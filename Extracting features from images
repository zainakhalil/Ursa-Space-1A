{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"11oy6q1Ld22X"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26312,"status":"ok","timestamp":1761148515523,"user":{"displayName":"Chiamanda Ononiwu","userId":"07565696795075931276"},"user_tz":300},"id":"v56ffXyWoRTR","outputId":"765624fa-ced7-451e-80eb-b3a2ef8f900b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AU6y4b7Gof-G"},"outputs":[],"source":["from google.colab import userdata\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8882,"status":"ok","timestamp":1760306893075,"user":{"displayName":"Chiamanda Ononiwu","userId":"07565696795075931276"},"user_tz":300},"id":"C4Lr0o2dbNJD","outputId":"15a6675f-e4f6-439d-f63e-74062fac6414"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers\u003c=0.23.0,\u003e=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors\u003e=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: pyarrow\u003e=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill\u003c0.3.9,\u003e=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess\u003c0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec\u003c=2025.3.0,\u003e=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (2025.3.0)\n","Requirement already satisfied: typing-extensions\u003e=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (3.13.0)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.34.0-\u003etransformers) (1.1.10)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (3.4.3)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003etransformers) (2025.10.5)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch) (3.0.3)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas-\u003edatasets) (2.9.0.post0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas-\u003edatasets) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas-\u003edatasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (1.4.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (25.4.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (1.8.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (6.7.0)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (0.3.2)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (1.22.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas-\u003edatasets) (1.17.0)\n"]}],"source":["# install transformer to get the trasnformer library in hugging face\n","# install torch since working with images (the PyTorch deep learning framework), and  Pillow for opening and editing images\n","# tqdm for progress bars\n","# datasets (Hugging Face library) tool that helps you load existing datasets, create your own datasets, Work efficiently with large data  without running out of memory\n","!pip install transformers datasets torch torchvision pillow tqdm\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KC_gntdwTkHH"},"source":["Using ViT Base pretrained on ImageNet as the first pretrained model for feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eA4FdjL9VMaa"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from transformers import AutoModel\n","import pickle\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ePT7LnCNI4J"},"outputs":[],"source":["#Preprocessing the data so it can be used by the ViT Base\n","def preprocess_bands(row, band1, band2):\n","  band1 = np.array(row[band1], dtype = np.float32)\n","  band2 = np.array(row[band2], dtype = np.float32)\n","  # stacking into 3 channels, since the pretrained model needs 3 channels\n","  img_array = np.stack([band1, band2, band1], axis = 2)\n","  img = Image.fromarray((img_array * 255).astype(np.uint8))\n","  return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUKGTapdOYeT"},"outputs":[],"source":["#Preprocessing to be used by PyTorch:\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),  # ViT input size\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                         std=[0.5, 0.5, 0.5])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmiOSYe1OtMG"},"outputs":[],"source":["# Making a Dataset class\n","class IcebergDataset(Dataset):\n","    def __init__(self, df, preprocess, band1, band2):\n","        self.df = df\n","        self.preprocess = preprocess  # transforms.Compose\n","        self.band1 = band1\n","        self.band2 = band2\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Preprocess bands into 3-channel PIL image\n","        band1 = np.array(row[self.band1], dtype=np.float32)\n","        band2 = np.array(row[self.band2], dtype=np.float32)\n","        img_array = np.stack([band1, band2, band1], axis=2)  # H x W x 3\n","        img = Image.fromarray((img_array * 255).astype(np.uint8))\n","\n","        # Apply timm / torchvision transforms\n","        img_tensor = self.preprocess(img)  # \u003c-- just call the transform\n","\n","        label = torch.tensor(row['is_iceberg'], dtype=torch.float32)\n","        return img_tensor, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1761150155541,"user":{"displayName":"Chiamanda Ononiwu","userId":"07565696795075931276"},"user_tz":300},"id":"gECWPJK1uq4K","outputId":"4445219c-1ff6-48e4-8326-f310f3b3a149"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02bc7bea1e2b4216b8c989222fe18995","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='\u003ccenter\u003e \u003cimg\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"]},"metadata":{},"output_type":"display_data"}],"source":["#from huggingface_hub import login\n","#login()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13592,"status":"ok","timestamp":1761587741901,"user":{"displayName":"Chiamanda Ononiwu","userId":"07565696795075931276"},"user_tz":300},"id":"ptQ0_Zx_SoJn","outputId":"8ea180a5-76c8-402e-e2fc-2b184347ca84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (3.20.0)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (2025.3.0)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (25.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (2.32.4)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (4.67.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (4.15.0)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub-\u003etimm) (1.1.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (75.2.0)\n","Requirement already satisfied: sympy\u003e=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch-\u003etimm) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision-\u003etimm) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision-\u003etimm) (11.3.0)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy\u003e=1.13.3-\u003etorch-\u003etimm) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-\u003etorch-\u003etimm) (3.0.3)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (3.4.4)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (3.11)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-\u003ehuggingface_hub-\u003etimm) (2025.10.5)\n"]}],"source":["!pip install timm\n","import timm\n","import torch\n","from PIL import Image\n","from torchvision import transforms\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":12321,"status":"ok","timestamp":1761587754245,"user":{"displayName":"Chiamanda Ononiwu","userId":"07565696795075931276"},"user_tz":300},"id":"xV029llcSJc5","outputId":"603bd3af-676f-417a-ccb7-c1a1d73f9885"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"982f2c4e64b2494b84122de9d7c14dee","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (patch_drop): Identity()\n","  (norm_pre): Identity()\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (q_norm): Identity()\n","        (k_norm): Identity()\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (norm): Identity()\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (fc_norm): Identity()\n","  (head_drop): Dropout(p=0.0, inplace=False)\n","  (head): Linear(in_features=768, out_features=1000, bias=True)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#Loading ViT Base\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = timm.create_model('vit_base_patch16_224', pretrained=True)\n","model.eval()\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58287,"status":"ok","timestamp":1761594377182,"user":{"displayName":"Chiamanda Ononiwu","userId":"07565696795075931276"},"user_tz":300},"id":"cUV8113RUUve","outputId":"432fa2be-b3f8-4f3b-b11b-7aa99eb4a421"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['id', 'band_1', 'band_2', 'inc_angle', 'is_iceberg', 'hv_hh_ratio',\n","       'total_power', 'ratio_mean', 'power_mean', 'band_1_mean', 'band_2_mean',\n","       'masked_band1', 'masked_band2'],\n","      dtype='object')\n","Index(['id', 'band_1', 'band_2', 'inc_angle', 'is_iceberg', 'hv_hh_ratio',\n","       'total_power', 'ratio_mean', 'power_mean', 'band_1_mean', 'band_2_mean',\n","       'band_1_ssf', 'band_2_ssf', 'hv_hh_ratio_ssf'],\n","      dtype='object')\n","Index(['id', 'band_1', 'band_2', 'inc_angle', 'is_iceberg', 'centroid_band1',\n","       'centroid_band2', 'zoom_band1', 'zoom_band2'],\n","      dtype='object')\n"]}],"source":["masked_training_df = pd.read_json(\"masked_training.json\")\n","print(masked_training_df.columns)\n","\n","ssf_training_df = pd.read_json(\"ssf_training.json\")\n","print(ssf_training_df.columns)\n","\n","zoom_training_df = pd.read_json(\"zoom_training.json\")\n","print(zoom_training_df.columns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49do_C2PqfJM"},"outputs":[],"source":["#Create data loader for the different preprocessed data\n","dataset_masked = IcebergDataset(masked_training_df, preprocess, 'masked_band1', 'masked_band2')\n","loader_masked = DataLoader(dataset_masked, batch_size=16, shuffle=False)\n","\n","dataset_ssf = IcebergDataset(ssf_training_df, preprocess, 'band_1_ssf', 'band_2_ssf')\n","loader_ssf = DataLoader(dataset_ssf, batch_size=16, shuffle=False)\n","\n","dataset_zoom = IcebergDataset(zoom_training_df, preprocess, 'zoom_band1', 'zoom_band2')\n","loader_zoom = DataLoader(dataset_zoom, batch_size=16, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXLIW-pQrDCz"},"outputs":[],"source":["def get_embedding(loader, device = device):\n","  all_features = []\n","  all_labels = []\n","\n","  for images, labels in tqdm(loader):\n","      images = images.to(device)\n","      with torch.no_grad():\n","          # Use timm's forward_features to get embeddings\n","          features = model.forward_features(images)  # shape [B, feature_dim]\n","\n","      all_features.append(features.cpu().numpy())\n","      all_labels.append(labels.numpy())\n","\n","  # Concatenate all batches\n","  all_features = np.concatenate(all_features, axis=0)\n","  all_labels = np.concatenate(all_labels, axis=0)\n","\n","  # Save features to pickle\n","  with open(\"train_features.pkl\", \"wb\") as f:\n","      pickle.dump({\"features\": all_features, \"labels\": all_labels}, f)\n","\n","  print(\"Feature extraction complete! Saved to train_features.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AVf_D2NTqP5","outputId":"d9667038-7af1-4833-9d6f-f80d037eaebd"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92/92 [19:19\u003c00:00, 12.60s/it]\n"]}],"source":["#get_embedding(loader_zoom, device = device)\n","get_embedding(loader_ssf, device = device)\n","get_embedding(loader_masked, device = device)\n"]},{"cell_type":"markdown","metadata":{"id":"aJaZAHWTUPN1"},"source":["swin-tiny-patch4-window7-224-finetuned-sar model\n","\n","\n","*  Swin Transformers are Vision Transformers (ViTs) that use a hierarchical, windowed self-attention mechanism.\n","\n","*  ‚ÄúTiny‚Äù = small version, efficient for faster training/inference.\n","\n","*  Patch size 4, window size 7 = defines how the image is split for attention computations.\n","\n","*  Input image size = 224√ó224 (standard for ImageNet pretraining).\n","\n","*  Fine-tuned on SAR backscatter patterns for icebergs vs. ships.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jP3KsUx_zlEA"},"outputs":[],"source":["from transformers import AutoImageProcessor, AutoModel\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","processor = AutoImageProcessor.from_pretrained(\"elifm/swin-tiny-patch4-window7-224-finetuned-sar\")\n","model = AutoModel.from_pretrained(\"elifm/swin-tiny-patch4-window7-224-finetuned-sar\")\n","model.eval()\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VS9pF0LBUSjk"},"outputs":[],"source":["#Loading Pretrained Swin Transformer\n","model_name2 = \"elifm/swin-tiny-patch4-window7-224-finetuned-sar\"\n","swin_processor = AutoImageProcessor.from_pretrained(model_name2) # resizes, normalizes, and converts\n","swin_model = AutoModel.from_pretrained(model_name2)\n","swin_model.eval() # don't update weights/no gradient updates\n","\n","# Move to GPU\n","swin_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBgQmqZlWbFJ"},"outputs":[],"source":["def get_embedding_swin(df, band1_col, band2_col, processor, model, device, dataset_name):\n","    all_features = []\n","    all_labels = []\n","\n","    print(\"Moving model to device and setting eval mode\")\n","    model.eval()\n","    model.to(device)\n","\n","    for i in tqdm(range(len(df)), desc=\"Processing images\"):\n","        try:\n","            print(f\"\\nüîπ Processing image {i+1}/{len(df)}\")\n","            # Select 1 image/row\n","            row = df.iloc[i]\n","\n","            # Convert bands to 3-channel PIL image\n","            img = preprocess_bands(row, band1_col, band2_col)\n","            print(f\"  Image shape after preprocessing: {np.array(img).shape}\")\n","\n","            # Process image (resizes to model's expected input size,\n","            # normalizes pixel values, and coverts Python-image-library (PIL)\n","            # image to PyTorch tensor (multidimensional-array))\n","            # Why? machine learning models require Tensor, not an img\n","            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n","            print(f\"  Input keys: {list(inputs.keys())}\")\n","            print(f\"  Input tensor shape: {inputs['pixel_values'].shape}\")\n","\n","            # Forward pass - Get embeddings\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","            print(f\"  Outputs type: {type(outputs)}\")\n","\n","            # Safely handle model outputs\n","            if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n","                features = outputs.pooler_output.cpu().numpy()\n","                print(f\"  Using pooler_output, shape: {features.shape}\")\n","            else:\n","                features = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n","                print(f\"  Using mean of last_hidden_state, shape: {features.shape}\")\n","\n","            # extract embeddings\n","            all_features.append(features)\n","            all_labels.append(row[\"is_iceberg\"])\n","\n","        except Exception as e:\n","            print(f\"Error processing image {i}: {e}\")\n","\n","    # Combine all embeddings and labels\n","    all_features = np.vstack(all_features)\n","    all_labels = np.array(all_labels)\n","    print(f\"\\nAll features stacked, shape: {all_features.shape}\")\n","    print(f\"All labels stacked, shape: {all_labels.shape}\")\n","\n","    # Save to pickle\n","    with open(f\"{dataset_name}_swin_features.pkl\", \"wb\") as f:\n","        pickle.dump({\"features\": all_features, \"labels\": all_labels}, f)\n","    print(f\"Swin feature extraction complete! Saved to {dataset_name}_swin_features.pkl\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"x4p4Lt3ubMQj"},"outputs":[{"name":"stdout","output_type":"stream","text":["Moving model to device and setting eval mode\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 0/1471 [00:00\u003c?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","üîπ Processing image 1/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 1/1471 [00:01\u003c28:58,  1.18s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 2/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 2/1471 [00:01\u003c17:19,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 3/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 3/1471 [00:01\u003c12:43,  1.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 4/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 4/1471 [00:02\u003c10:52,  2.25it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 5/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 5/1471 [00:02\u003c09:45,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 6/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 6/1471 [00:02\u003c09:01,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 7/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   0%|          | 7/1471 [00:03\u003c08:39,  2.82it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 8/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 8/1471 [00:03\u003c08:20,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 9/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 9/1471 [00:03\u003c08:07,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 10/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 10/1471 [00:04\u003c08:01,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 11/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 11/1471 [00:04\u003c08:04,  3.01it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 12/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 12/1471 [00:04\u003c08:03,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 13/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 13/1471 [00:05\u003c07:59,  3.04it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 14/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 14/1471 [00:05\u003c08:09,  2.98it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 15/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 15/1471 [00:05\u003c08:31,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 16/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 16/1471 [00:06\u003c08:32,  2.84it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 17/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 17/1471 [00:06\u003c08:29,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 18/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|          | 18/1471 [00:06\u003c08:13,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 19/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|‚ñè         | 19/1471 [00:07\u003c08:11,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 20/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|‚ñè         | 20/1471 [00:07\u003c08:16,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 21/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|‚ñè         | 21/1471 [00:07\u003c08:10,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 22/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   1%|‚ñè         | 22/1471 [00:08\u003c08:07,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 23/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 23/1471 [00:08\u003c08:12,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 24/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 24/1471 [00:08\u003c08:12,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 25/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 25/1471 [00:09\u003c08:15,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 26/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 26/1471 [00:09\u003c08:31,  2.82it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 27/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 27/1471 [00:09\u003c08:26,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 28/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 28/1471 [00:10\u003c08:26,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 29/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 29/1471 [00:10\u003c08:24,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 30/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 30/1471 [00:10\u003c08:21,  2.87it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 31/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 31/1471 [00:11\u003c08:26,  2.84it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 32/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 32/1471 [00:11\u003c08:35,  2.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 33/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 33/1471 [00:12\u003c09:49,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 34/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 34/1471 [00:12\u003c12:09,  1.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 35/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 35/1471 [00:13\u003c13:46,  1.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 36/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   2%|‚ñè         | 36/1471 [00:14\u003c12:55,  1.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 37/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 37/1471 [00:14\u003c12:29,  1.91it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 38/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 38/1471 [00:15\u003c11:32,  2.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 39/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 39/1471 [00:15\u003c10:58,  2.17it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 40/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 40/1471 [00:15\u003c09:46,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 41/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 41/1471 [00:16\u003c09:05,  2.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 42/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 42/1471 [00:16\u003c08:28,  2.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 43/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 43/1471 [00:16\u003c07:51,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 44/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 44/1471 [00:16\u003c07:24,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 45/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 45/1471 [00:17\u003c07:15,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 46/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 46/1471 [00:17\u003c07:01,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 47/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 47/1471 [00:17\u003c06:49,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 48/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 48/1471 [00:17\u003c06:39,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 49/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 49/1471 [00:18\u003c06:33,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 50/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 50/1471 [00:18\u003c06:26,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 51/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   3%|‚ñé         | 51/1471 [00:18\u003c06:21,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 52/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñé         | 52/1471 [00:19\u003c06:18,  3.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 53/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñé         | 53/1471 [00:19\u003c06:21,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 54/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñé         | 54/1471 [00:19\u003c06:21,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 55/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñé         | 55/1471 [00:19\u003c06:18,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 56/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 56/1471 [00:20\u003c06:22,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 57/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 57/1471 [00:20\u003c06:19,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 58/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 58/1471 [00:20\u003c06:17,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 59/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 59/1471 [00:20\u003c06:11,  3.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 60/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 60/1471 [00:21\u003c06:18,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 61/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 61/1471 [00:21\u003c06:21,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 62/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 62/1471 [00:21\u003c06:19,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 63/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 63/1471 [00:22\u003c06:16,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 64/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 64/1471 [00:22\u003c06:24,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 65/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 65/1471 [00:22\u003c06:19,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 66/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   4%|‚ñç         | 66/1471 [00:22\u003c06:18,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 67/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 67/1471 [00:23\u003c06:12,  3.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 68/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 68/1471 [00:23\u003c06:16,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 69/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 69/1471 [00:23\u003c06:17,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 70/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 70/1471 [00:23\u003c06:14,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 71/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 71/1471 [00:24\u003c06:10,  3.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 72/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 72/1471 [00:24\u003c06:14,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 73/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñç         | 73/1471 [00:24\u003c06:26,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 74/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 74/1471 [00:25\u003c07:07,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 75/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 75/1471 [00:25\u003c07:39,  3.04it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 76/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 76/1471 [00:25\u003c08:07,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 77/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 77/1471 [00:26\u003c08:12,  2.83it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 78/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 78/1471 [00:26\u003c08:33,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 79/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 79/1471 [00:27\u003c08:54,  2.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 80/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   5%|‚ñå         | 80/1471 [00:27\u003c08:58,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 81/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 81/1471 [00:27\u003c09:04,  2.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 82/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 82/1471 [00:28\u003c08:48,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 83/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 83/1471 [00:28\u003c08:25,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 84/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 84/1471 [00:28\u003c08:15,  2.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 85/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 85/1471 [00:29\u003c08:00,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 86/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 86/1471 [00:29\u003c07:45,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 87/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 87/1471 [00:29\u003c07:48,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 88/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 88/1471 [00:30\u003c07:41,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 89/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 89/1471 [00:30\u003c07:37,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 90/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 90/1471 [00:30\u003c07:48,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 91/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñå         | 91/1471 [00:31\u003c07:42,  2.98it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 92/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñã         | 92/1471 [00:31\u003c07:43,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 93/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñã         | 93/1471 [00:31\u003c07:46,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 94/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñã         | 94/1471 [00:32\u003c07:44,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 95/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   6%|‚ñã         | 95/1471 [00:32\u003c07:44,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 96/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 96/1471 [00:32\u003c07:37,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 97/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 97/1471 [00:33\u003c07:32,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 98/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 98/1471 [00:33\u003c07:20,  3.11it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 99/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 99/1471 [00:33\u003c07:34,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 100/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 100/1471 [00:34\u003c07:37,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 101/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 101/1471 [00:34\u003c07:32,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 102/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 102/1471 [00:34\u003c07:36,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 103/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 103/1471 [00:35\u003c07:38,  2.98it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 104/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 104/1471 [00:35\u003c07:33,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 105/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 105/1471 [00:35\u003c07:30,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 106/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 106/1471 [00:36\u003c07:53,  2.88it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 107/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 107/1471 [00:36\u003c08:02,  2.83it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 108/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 108/1471 [00:37\u003c08:26,  2.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 109/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 109/1471 [00:37\u003c08:25,  2.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 110/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   7%|‚ñã         | 110/1471 [00:37\u003c08:20,  2.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 111/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 111/1471 [00:38\u003c08:11,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 112/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 112/1471 [00:38\u003c08:06,  2.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 113/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 113/1471 [00:39\u003c13:56,  1.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 114/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 114/1471 [00:40\u003c12:20,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 115/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 115/1471 [00:40\u003c10:31,  2.15it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 116/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 116/1471 [00:40\u003c09:11,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 117/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 117/1471 [00:40\u003c08:32,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 118/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 118/1471 [00:41\u003c07:43,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 119/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 119/1471 [00:41\u003c07:10,  3.14it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 120/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 120/1471 [00:41\u003c06:47,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 121/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 121/1471 [00:41\u003c06:39,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 122/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 122/1471 [00:42\u003c06:29,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 123/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 123/1471 [00:42\u003c06:20,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 124/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 124/1471 [00:42\u003c06:10,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 125/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   8%|‚ñä         | 125/1471 [00:43\u003c06:10,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 126/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñä         | 126/1471 [00:43\u003c06:02,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 127/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñä         | 127/1471 [00:43\u003c05:57,  3.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 128/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñä         | 128/1471 [00:43\u003c05:56,  3.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 129/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 129/1471 [00:44\u003c06:04,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 130/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 130/1471 [00:44\u003c05:56,  3.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 131/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 131/1471 [00:44\u003c05:53,  3.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 132/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 132/1471 [00:44\u003c05:49,  3.83it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 133/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 133/1471 [00:45\u003c05:54,  3.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 134/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 134/1471 [00:45\u003c05:53,  3.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 135/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 135/1471 [00:45\u003c05:50,  3.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 136/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 136/1471 [00:45\u003c05:46,  3.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 137/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 137/1471 [00:46\u003c05:58,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 138/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 138/1471 [00:46\u003c05:54,  3.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 139/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:   9%|‚ñâ         | 139/1471 [00:46\u003c05:50,  3.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 140/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 140/1471 [00:46\u003c05:50,  3.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 141/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 141/1471 [00:47\u003c05:59,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 142/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 142/1471 [00:47\u003c05:54,  3.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 143/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 143/1471 [00:47\u003c05:56,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 144/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 144/1471 [00:48\u003c05:54,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 145/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 145/1471 [00:48\u003c05:54,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 146/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 146/1471 [00:48\u003c05:52,  3.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 147/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñâ         | 147/1471 [00:48\u003c05:50,  3.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 148/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 148/1471 [00:49\u003c05:50,  3.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 149/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 149/1471 [00:49\u003c05:56,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 150/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 150/1471 [00:49\u003c05:54,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 151/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 151/1471 [00:49\u003c06:12,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 152/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 152/1471 [00:50\u003c06:42,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 153/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 153/1471 [00:50\u003c07:03,  3.11it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 154/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  10%|‚ñà         | 154/1471 [00:51\u003c07:06,  3.08it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 155/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 155/1471 [00:51\u003c07:29,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 156/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 156/1471 [00:51\u003c07:30,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 157/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 157/1471 [00:52\u003c07:37,  2.87it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 158/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 158/1471 [00:52\u003c07:55,  2.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 159/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 159/1471 [00:52\u003c08:05,  2.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 160/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 160/1471 [00:53\u003c08:06,  2.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 161/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 161/1471 [00:53\u003c08:15,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 162/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 162/1471 [00:54\u003c07:59,  2.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 163/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 163/1471 [00:54\u003c07:49,  2.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 164/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 164/1471 [00:54\u003c07:46,  2.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 165/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà         | 165/1471 [00:55\u003c07:32,  2.88it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 166/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà‚ñè        | 166/1471 [00:55\u003c07:22,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 167/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà‚ñè        | 167/1471 [00:55\u003c07:35,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 168/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà‚ñè        | 168/1471 [00:56\u003c07:51,  2.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 169/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  11%|‚ñà‚ñè        | 169/1471 [00:56\u003c08:00,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 170/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 170/1471 [00:56\u003c07:56,  2.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 171/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 171/1471 [00:57\u003c08:04,  2.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 172/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 172/1471 [00:57\u003c07:58,  2.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 173/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 173/1471 [00:57\u003c07:20,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 174/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 174/1471 [00:58\u003c06:50,  3.16it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 175/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 175/1471 [00:58\u003c06:27,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 176/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 176/1471 [00:58\u003c06:14,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 177/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 177/1471 [00:58\u003c06:10,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 178/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 178/1471 [00:59\u003c06:00,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 179/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 179/1471 [00:59\u003c05:53,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 180/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 180/1471 [00:59\u003c05:50,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 181/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 181/1471 [00:59\u003c05:51,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 182/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 182/1471 [01:00\u003c05:47,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 183/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  12%|‚ñà‚ñè        | 183/1471 [01:00\u003c05:43,  3.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 184/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 184/1471 [01:00\u003c05:40,  3.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 185/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 185/1471 [01:01\u003c05:43,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 186/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 186/1471 [01:01\u003c05:37,  3.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 187/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 187/1471 [01:01\u003c05:38,  3.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 188/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 188/1471 [01:01\u003c05:39,  3.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 189/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 189/1471 [01:02\u003c05:54,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 190/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 190/1471 [01:02\u003c05:57,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 191/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 191/1471 [01:02\u003c05:50,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 192/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 192/1471 [01:02\u003c05:45,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 193/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 193/1471 [01:03\u003c05:47,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 194/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 194/1471 [01:03\u003c05:41,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 195/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 195/1471 [01:03\u003c05:39,  3.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 196/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 196/1471 [01:04\u003c05:35,  3.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 197/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 197/1471 [01:04\u003c05:44,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 198/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  13%|‚ñà‚ñé        | 198/1471 [01:04\u003c05:43,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 199/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñé        | 199/1471 [01:04\u003c05:42,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 200/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñé        | 200/1471 [01:05\u003c05:39,  3.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 201/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñé        | 201/1471 [01:05\u003c05:42,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 202/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñé        | 202/1471 [01:05\u003c05:43,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 203/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 203/1471 [01:05\u003c05:41,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 204/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 204/1471 [01:06\u003c06:06,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 205/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 205/1471 [01:06\u003c05:54,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 206/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 206/1471 [01:06\u003c05:48,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 207/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 207/1471 [01:07\u003c05:46,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 208/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 208/1471 [01:07\u003c05:50,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 209/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 209/1471 [01:07\u003c05:54,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 210/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 210/1471 [01:07\u003c06:33,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 211/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 211/1471 [01:08\u003c06:55,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 212/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 212/1471 [01:08\u003c07:03,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 213/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  14%|‚ñà‚ñç        | 213/1471 [01:09\u003c07:12,  2.91it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 214/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 214/1471 [01:09\u003c07:34,  2.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 215/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 215/1471 [01:09\u003c07:43,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 216/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 216/1471 [01:10\u003c07:44,  2.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 217/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 217/1471 [01:10\u003c07:53,  2.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 218/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 218/1471 [01:11\u003c08:08,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 219/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 219/1471 [01:11\u003c07:51,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 220/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñç        | 220/1471 [01:11\u003c07:50,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 221/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 221/1471 [01:12\u003c07:36,  2.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 222/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 222/1471 [01:13\u003c14:39,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 223/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 223/1471 [01:15\u003c23:39,  1.14s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 224/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 224/1471 [01:17\u003c25:43,  1.24s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 225/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 225/1471 [01:17\u003c22:21,  1.08s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 226/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 226/1471 [01:18\u003c17:52,  1.16it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 227/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 227/1471 [01:18\u003c14:49,  1.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 228/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  15%|‚ñà‚ñå        | 228/1471 [01:18\u003c12:18,  1.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 229/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 229/1471 [01:19\u003c10:21,  2.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 230/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 230/1471 [01:19\u003c08:51,  2.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 231/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 231/1471 [01:19\u003c07:48,  2.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 232/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 232/1471 [01:20\u003c07:25,  2.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 233/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 233/1471 [01:20\u003c06:55,  2.98it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 234/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 234/1471 [01:20\u003c06:31,  3.16it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 235/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 235/1471 [01:20\u003c06:12,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 236/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 236/1471 [01:21\u003c06:02,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 237/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 237/1471 [01:21\u003c06:05,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 238/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 238/1471 [01:21\u003c05:57,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 239/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñå        | 239/1471 [01:22\u003c05:54,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 240/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñã        | 240/1471 [01:22\u003c06:02,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 241/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñã        | 241/1471 [01:22\u003c05:53,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 242/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  16%|‚ñà‚ñã        | 242/1471 [01:22\u003c05:45,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 243/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 243/1471 [01:23\u003c05:42,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 244/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 244/1471 [01:23\u003c05:48,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 245/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 245/1471 [01:23\u003c05:43,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 246/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 246/1471 [01:24\u003c05:45,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 247/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 247/1471 [01:24\u003c05:51,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 248/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 248/1471 [01:24\u003c05:54,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 249/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 249/1471 [01:24\u003c05:53,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 250/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 250/1471 [01:25\u003c05:48,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 251/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 251/1471 [01:25\u003c05:43,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 252/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 252/1471 [01:25\u003c05:51,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 253/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 253/1471 [01:26\u003c05:51,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 254/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 254/1471 [01:26\u003c05:49,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 255/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 255/1471 [01:26\u003c05:51,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 256/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 256/1471 [01:26\u003c05:48,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 257/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  17%|‚ñà‚ñã        | 257/1471 [01:27\u003c05:58,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 258/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 258/1471 [01:27\u003c05:59,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 259/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 259/1471 [01:27\u003c05:59,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 260/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 260/1471 [01:28\u003c05:55,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 261/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 261/1471 [01:28\u003c05:47,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 262/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 262/1471 [01:28\u003c05:45,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 263/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 263/1471 [01:28\u003c05:59,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 264/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 264/1471 [01:29\u003c07:02,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 265/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 265/1471 [01:31\u003c15:22,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 266/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 266/1471 [01:32\u003c21:32,  1.07s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 267/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 267/1471 [01:35\u003c27:21,  1.36s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 268/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 268/1471 [01:35\u003c23:42,  1.18s/it]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 269/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 269/1471 [01:36\u003c18:24,  1.09it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 270/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 270/1471 [01:36\u003c14:52,  1.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 271/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 271/1471 [01:36\u003c12:07,  1.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 272/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  18%|‚ñà‚ñä        | 272/1471 [01:36\u003c10:09,  1.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 273/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñä        | 273/1471 [01:37\u003c08:49,  2.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 274/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñä        | 274/1471 [01:37\u003c07:53,  2.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 275/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñä        | 275/1471 [01:37\u003c07:05,  2.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 276/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 276/1471 [01:38\u003c06:35,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 277/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 277/1471 [01:38\u003c06:13,  3.20it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 278/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 278/1471 [01:38\u003c06:06,  3.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 279/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 279/1471 [01:38\u003c05:54,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 280/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 280/1471 [01:39\u003c05:45,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 281/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 281/1471 [01:39\u003c05:43,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 282/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 282/1471 [01:39\u003c05:41,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 283/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 283/1471 [01:40\u003c05:36,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 284/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 284/1471 [01:40\u003c05:28,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 285/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 285/1471 [01:40\u003c05:27,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 286/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  19%|‚ñà‚ñâ        | 286/1471 [01:40\u003c05:33,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 287/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 287/1471 [01:41\u003c05:34,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 288/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 288/1471 [01:41\u003c05:31,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 289/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 289/1471 [01:41\u003c05:36,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 290/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 290/1471 [01:42\u003c05:33,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 291/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 291/1471 [01:42\u003c05:35,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 292/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 292/1471 [01:42\u003c05:35,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 293/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 293/1471 [01:42\u003c05:37,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 294/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñâ        | 294/1471 [01:43\u003c05:32,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 295/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 295/1471 [01:43\u003c05:29,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 296/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 296/1471 [01:43\u003c05:26,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 297/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 297/1471 [01:43\u003c05:31,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 298/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 298/1471 [01:44\u003c05:28,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 299/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 299/1471 [01:44\u003c05:27,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 300/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 300/1471 [01:44\u003c05:55,  3.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 301/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  20%|‚ñà‚ñà        | 301/1471 [01:45\u003c06:29,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 302/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 302/1471 [01:45\u003c06:56,  2.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 303/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 303/1471 [01:46\u003c07:16,  2.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 304/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 304/1471 [01:46\u003c07:26,  2.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 305/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 305/1471 [01:46\u003c07:35,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 306/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 306/1471 [01:47\u003c07:42,  2.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 307/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 307/1471 [01:47\u003c07:53,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 308/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 308/1471 [01:48\u003c08:03,  2.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 309/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 309/1471 [01:48\u003c07:47,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 310/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 310/1471 [01:48\u003c07:01,  2.76it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 311/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 311/1471 [01:49\u003c06:28,  2.99it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 312/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà        | 312/1471 [01:49\u003c06:11,  3.12it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 313/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà‚ñè       | 313/1471 [01:49\u003c05:54,  3.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 314/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà‚ñè       | 314/1471 [01:49\u003c05:41,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 315/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà‚ñè       | 315/1471 [01:50\u003c05:36,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 316/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  21%|‚ñà‚ñà‚ñè       | 316/1471 [01:50\u003c05:34,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 317/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 317/1471 [01:50\u003c05:26,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 318/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 318/1471 [01:51\u003c05:22,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 319/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 319/1471 [01:51\u003c05:25,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 320/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 320/1471 [01:51\u003c05:24,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 321/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 321/1471 [01:51\u003c05:20,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 322/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 322/1471 [01:52\u003c05:21,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 323/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 323/1471 [01:52\u003c05:22,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 324/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 324/1471 [01:52\u003c05:18,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 325/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 325/1471 [01:53\u003c05:17,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 326/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 326/1471 [01:53\u003c05:14,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 327/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 327/1471 [01:53\u003c05:16,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 328/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 328/1471 [01:53\u003c05:14,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 329/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 329/1471 [01:54\u003c05:11,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 330/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  22%|‚ñà‚ñà‚ñè       | 330/1471 [01:54\u003c05:11,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 331/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 331/1471 [01:54\u003c05:15,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 332/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 332/1471 [01:54\u003c05:12,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 333/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 333/1471 [01:55\u003c05:09,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 334/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 334/1471 [01:55\u003c05:08,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 335/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 335/1471 [01:55\u003c05:18,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 336/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 336/1471 [01:56\u003c05:21,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 337/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 337/1471 [01:56\u003c05:33,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 338/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 338/1471 [01:56\u003c05:31,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 339/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 339/1471 [01:56\u003c05:22,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 340/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 340/1471 [01:57\u003c05:22,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 341/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 341/1471 [01:57\u003c05:17,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 342/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 342/1471 [01:57\u003c05:32,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 343/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 343/1471 [01:58\u003c05:22,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 344/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 344/1471 [01:58\u003c05:16,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 345/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  23%|‚ñà‚ñà‚ñé       | 345/1471 [01:58\u003c05:36,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 346/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñé       | 346/1471 [01:59\u003c06:08,  3.05it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 347/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñé       | 347/1471 [01:59\u003c06:37,  2.82it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 348/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñé       | 348/1471 [01:59\u003c07:05,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 349/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñé       | 349/1471 [02:00\u003c07:13,  2.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 350/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 350/1471 [02:00\u003c07:21,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 351/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 351/1471 [02:01\u003c07:29,  2.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 352/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 352/1471 [02:01\u003c07:36,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 353/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 353/1471 [02:02\u003c07:40,  2.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 354/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 354/1471 [02:02\u003c07:25,  2.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 355/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 355/1471 [02:02\u003c06:42,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 356/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 356/1471 [02:02\u003c06:11,  3.00it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 357/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 357/1471 [02:03\u003c05:53,  3.15it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 358/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 358/1471 [02:03\u003c05:36,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 359/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 359/1471 [02:03\u003c05:27,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 360/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  24%|‚ñà‚ñà‚ñç       | 360/1471 [02:04\u003c05:20,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 361/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 361/1471 [02:04\u003c05:15,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 362/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 362/1471 [02:04\u003c05:12,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 363/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 363/1471 [02:04\u003c05:09,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 364/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 364/1471 [02:05\u003c05:06,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 365/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 365/1471 [02:05\u003c05:09,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 366/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 366/1471 [02:05\u003c05:09,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 367/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñç       | 367/1471 [02:05\u003c05:06,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 368/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 368/1471 [02:06\u003c05:08,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 369/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 369/1471 [02:06\u003c05:15,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 370/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 370/1471 [02:06\u003c05:09,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 371/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 371/1471 [02:07\u003c05:08,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 372/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 372/1471 [02:07\u003c05:09,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 373/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 373/1471 [02:07\u003c05:06,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 374/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 374/1471 [02:07\u003c05:02,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 375/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  25%|‚ñà‚ñà‚ñå       | 375/1471 [02:08\u003c05:00,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 376/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 376/1471 [02:08\u003c05:00,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 377/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 377/1471 [02:08\u003c04:58,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 378/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 378/1471 [02:09\u003c04:59,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 379/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 379/1471 [02:09\u003c04:57,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 380/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 380/1471 [02:09\u003c05:00,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 381/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 381/1471 [02:09\u003c04:59,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 382/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 382/1471 [02:10\u003c04:56,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 383/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 383/1471 [02:10\u003c04:56,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 384/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 384/1471 [02:10\u003c04:53,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 385/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 385/1471 [02:10\u003c04:53,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 386/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñå       | 386/1471 [02:11\u003c04:51,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 387/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñã       | 387/1471 [02:11\u003c04:56,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 388/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñã       | 388/1471 [02:11\u003c04:56,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 389/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  26%|‚ñà‚ñà‚ñã       | 389/1471 [02:12\u003c04:55,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 390/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 390/1471 [02:12\u003c04:56,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 391/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 391/1471 [02:12\u003c05:45,  3.13it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 392/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 392/1471 [02:13\u003c06:14,  2.88it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 393/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 393/1471 [02:13\u003c06:31,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 394/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 394/1471 [02:13\u003c06:47,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 395/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 395/1471 [02:14\u003c06:52,  2.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 396/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 396/1471 [02:14\u003c07:01,  2.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 397/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 397/1471 [02:15\u003c07:09,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 398/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 398/1471 [02:15\u003c07:17,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 399/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 399/1471 [02:16\u003c07:30,  2.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 400/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 400/1471 [02:16\u003c06:43,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 401/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 401/1471 [02:16\u003c06:10,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 402/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 402/1471 [02:16\u003c05:48,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 403/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 403/1471 [02:17\u003c05:33,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 404/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  27%|‚ñà‚ñà‚ñã       | 404/1471 [02:17\u003c05:24,  3.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 405/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 405/1471 [02:17\u003c05:12,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 406/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 406/1471 [02:17\u003c05:11,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 407/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 407/1471 [02:18\u003c05:02,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 408/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 408/1471 [02:18\u003c04:55,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 409/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 409/1471 [02:18\u003c04:53,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 410/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 410/1471 [02:19\u003c04:57,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 411/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 411/1471 [02:19\u003c04:51,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 412/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 412/1471 [02:19\u003c04:48,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 413/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 413/1471 [02:19\u003c04:49,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 414/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 414/1471 [02:20\u003c04:53,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 415/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 415/1471 [02:20\u003c04:50,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 416/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 416/1471 [02:20\u003c04:46,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 417/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 417/1471 [02:20\u003c04:44,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 418/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 418/1471 [02:21\u003c04:47,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 419/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  28%|‚ñà‚ñà‚ñä       | 419/1471 [02:21\u003c04:48,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 420/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñä       | 420/1471 [02:21\u003c04:47,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 421/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñä       | 421/1471 [02:22\u003c04:45,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 422/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñä       | 422/1471 [02:22\u003c04:49,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 423/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 423/1471 [02:22\u003c04:45,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 424/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 424/1471 [02:22\u003c04:41,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 425/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 425/1471 [02:23\u003c04:46,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 426/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 426/1471 [02:23\u003c04:42,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 427/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 427/1471 [02:23\u003c04:39,  3.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 428/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 428/1471 [02:23\u003c04:39,  3.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 429/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 429/1471 [02:24\u003c04:43,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 430/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 430/1471 [02:24\u003c04:40,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 431/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 431/1471 [02:24\u003c04:41,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 432/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 432/1471 [02:25\u003c04:39,  3.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 433/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  29%|‚ñà‚ñà‚ñâ       | 433/1471 [02:25\u003c04:40,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 434/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 434/1471 [02:25\u003c04:40,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 435/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 435/1471 [02:25\u003c04:42,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 436/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 436/1471 [02:26\u003c04:55,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 437/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 437/1471 [02:26\u003c05:32,  3.11it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 438/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 438/1471 [02:26\u003c05:56,  2.90it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 439/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 439/1471 [02:27\u003c06:21,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 440/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 440/1471 [02:27\u003c06:33,  2.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 441/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñâ       | 441/1471 [02:28\u003c06:38,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 442/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 442/1471 [02:28\u003c06:49,  2.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 443/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 443/1471 [02:29\u003c06:56,  2.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 444/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 444/1471 [02:29\u003c07:02,  2.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 445/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 445/1471 [02:29\u003c07:00,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 446/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 446/1471 [02:30\u003c06:25,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 447/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 447/1471 [02:30\u003c05:57,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 448/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  30%|‚ñà‚ñà‚ñà       | 448/1471 [02:30\u003c05:38,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 449/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 449/1471 [02:31\u003c05:20,  3.19it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 450/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 450/1471 [02:31\u003c05:07,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 451/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 451/1471 [02:31\u003c04:59,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 452/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 452/1471 [02:31\u003c04:54,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 453/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 453/1471 [02:32\u003c04:52,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 454/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 454/1471 [02:32\u003c04:48,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 455/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 455/1471 [02:32\u003c04:44,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 456/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 456/1471 [02:32\u003c04:45,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 457/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 457/1471 [02:33\u003c04:40,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 458/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 458/1471 [02:33\u003c04:35,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 459/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà       | 459/1471 [02:33\u003c04:33,  3.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 460/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà‚ñè      | 460/1471 [02:34\u003c04:37,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 461/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà‚ñè      | 461/1471 [02:34\u003c04:34,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 462/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà‚ñè      | 462/1471 [02:34\u003c04:35,  3.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 463/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  31%|‚ñà‚ñà‚ñà‚ñè      | 463/1471 [02:34\u003c04:40,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 464/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 464/1471 [02:35\u003c04:38,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 465/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 465/1471 [02:35\u003c04:35,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 466/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 466/1471 [02:35\u003c04:36,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 467/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 467/1471 [02:36\u003c04:42,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 468/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 468/1471 [02:36\u003c04:42,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 469/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 469/1471 [02:36\u003c04:47,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 470/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 470/1471 [02:36\u003c04:42,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 471/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 471/1471 [02:37\u003c04:47,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 472/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 472/1471 [02:37\u003c04:46,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 473/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 473/1471 [02:37\u003c04:41,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 474/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 474/1471 [02:38\u003c04:41,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 475/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 475/1471 [02:38\u003c04:39,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 476/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 476/1471 [02:38\u003c04:38,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 477/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 477/1471 [02:38\u003c04:35,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 478/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  32%|‚ñà‚ñà‚ñà‚ñè      | 478/1471 [02:39\u003c04:39,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 479/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 479/1471 [02:39\u003c04:42,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 480/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 480/1471 [02:39\u003c04:38,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 481/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 481/1471 [02:40\u003c07:04,  2.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 482/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 482/1471 [02:41\u003c10:52,  1.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 483/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 483/1471 [02:42\u003c11:42,  1.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 484/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 484/1471 [02:43\u003c10:47,  1.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 485/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 485/1471 [02:43\u003c09:28,  1.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 486/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 486/1471 [02:43\u003c08:32,  1.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 487/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 487/1471 [02:44\u003c07:57,  2.06it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 488/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 488/1471 [02:44\u003c07:36,  2.16it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 489/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 489/1471 [02:45\u003c07:25,  2.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 490/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 490/1471 [02:45\u003c07:11,  2.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 491/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 491/1471 [02:45\u003c06:26,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 492/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  33%|‚ñà‚ñà‚ñà‚ñé      | 492/1471 [02:46\u003c05:53,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 493/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñé      | 493/1471 [02:46\u003c05:35,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 494/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñé      | 494/1471 [02:46\u003c05:18,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 495/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñé      | 495/1471 [02:46\u003c05:08,  3.17it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 496/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñé      | 496/1471 [02:47\u003c05:14,  3.10it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 497/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 497/1471 [02:47\u003c05:09,  3.14it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 498/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 498/1471 [02:47\u003c04:58,  3.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 499/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 499/1471 [02:48\u003c04:50,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 500/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 500/1471 [02:48\u003c04:49,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 501/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 501/1471 [02:48\u003c04:43,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 502/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 502/1471 [02:48\u003c04:39,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 503/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 503/1471 [02:49\u003c04:42,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 504/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 504/1471 [02:49\u003c04:38,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 505/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 505/1471 [02:49\u003c04:35,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 506/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 506/1471 [02:50\u003c04:33,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 507/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  34%|‚ñà‚ñà‚ñà‚ñç      | 507/1471 [02:50\u003c04:43,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 508/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 508/1471 [02:50\u003c04:36,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 509/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 509/1471 [02:50\u003c04:31,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 510/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 510/1471 [02:51\u003c04:36,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 511/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 511/1471 [02:51\u003c04:52,  3.28it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 512/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 512/1471 [02:51\u003c04:43,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 513/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 513/1471 [02:52\u003c04:45,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 514/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñç      | 514/1471 [02:52\u003c04:46,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 515/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 515/1471 [02:52\u003c04:40,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 516/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 516/1471 [02:53\u003c04:36,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 517/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 517/1471 [02:53\u003c04:35,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 518/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 518/1471 [02:53\u003c04:38,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 519/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 519/1471 [02:53\u003c04:43,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 520/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 520/1471 [02:54\u003c04:46,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 521/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 521/1471 [02:54\u003c04:55,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 522/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  35%|‚ñà‚ñà‚ñà‚ñå      | 522/1471 [02:54\u003c04:50,  3.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 523/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 523/1471 [02:55\u003c04:46,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 524/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 524/1471 [02:55\u003c04:46,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 525/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 525/1471 [02:55\u003c05:36,  2.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 526/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 526/1471 [02:56\u003c06:04,  2.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 527/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 527/1471 [02:56\u003c06:20,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 528/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 528/1471 [02:57\u003c06:29,  2.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 529/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 529/1471 [02:57\u003c06:43,  2.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 530/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 530/1471 [02:58\u003c06:53,  2.28it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 531/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 531/1471 [02:58\u003c06:47,  2.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 532/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 532/1471 [02:59\u003c06:46,  2.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 533/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñå      | 533/1471 [02:59\u003c06:22,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 534/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñã      | 534/1471 [02:59\u003c05:44,  2.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 535/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñã      | 535/1471 [02:59\u003c05:16,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 536/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  36%|‚ñà‚ñà‚ñà‚ñã      | 536/1471 [03:00\u003c05:04,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 537/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 537/1471 [03:00\u003c04:56,  3.15it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 538/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 538/1471 [03:00\u003c04:44,  3.28it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 539/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 539/1471 [03:01\u003c04:38,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 540/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 540/1471 [03:01\u003c04:34,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 541/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 541/1471 [03:01\u003c04:27,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 542/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 542/1471 [03:01\u003c04:22,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 543/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 543/1471 [03:02\u003c04:27,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 544/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 544/1471 [03:02\u003c04:24,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 545/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 545/1471 [03:02\u003c04:20,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 546/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 546/1471 [03:03\u003c04:17,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 547/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 547/1471 [03:03\u003c04:19,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 548/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 548/1471 [03:03\u003c04:16,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 549/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 549/1471 [03:03\u003c04:17,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 550/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 550/1471 [03:04\u003c04:21,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 551/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  37%|‚ñà‚ñà‚ñà‚ñã      | 551/1471 [03:04\u003c04:22,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 552/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 552/1471 [03:04\u003c04:22,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 553/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 553/1471 [03:05\u003c04:20,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 554/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 554/1471 [03:05\u003c04:26,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 555/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 555/1471 [03:05\u003c04:25,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 556/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 556/1471 [03:05\u003c04:29,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 557/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 557/1471 [03:06\u003c04:30,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 558/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 558/1471 [03:06\u003c04:35,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 559/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 559/1471 [03:06\u003c04:38,  3.28it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 560/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 560/1471 [03:07\u003c04:38,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 561/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 561/1471 [03:07\u003c04:40,  3.25it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 562/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 562/1471 [03:07\u003c04:35,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 563/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 563/1471 [03:08\u003c04:30,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 564/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 564/1471 [03:08\u003c04:24,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 565/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 565/1471 [03:08\u003c04:25,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 566/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  38%|‚ñà‚ñà‚ñà‚ñä      | 566/1471 [03:08\u003c04:19,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 567/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñä      | 567/1471 [03:09\u003c04:15,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 568/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñä      | 568/1471 [03:09\u003c04:40,  3.22it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 569/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñä      | 569/1471 [03:09\u003c05:08,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 570/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñä      | 570/1471 [03:10\u003c05:25,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 571/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 571/1471 [03:10\u003c05:36,  2.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 572/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 572/1471 [03:11\u003c05:49,  2.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 573/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 573/1471 [03:11\u003c05:58,  2.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 574/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 574/1471 [03:12\u003c06:06,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 575/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 575/1471 [03:12\u003c06:09,  2.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 576/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 576/1471 [03:12\u003c06:19,  2.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 577/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 577/1471 [03:13\u003c05:45,  2.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 578/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 578/1471 [03:13\u003c05:13,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 579/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 579/1471 [03:13\u003c04:58,  2.99it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 580/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 580/1471 [03:14\u003c04:45,  3.12it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 581/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  39%|‚ñà‚ñà‚ñà‚ñâ      | 581/1471 [03:14\u003c04:37,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 582/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 582/1471 [03:14\u003c04:32,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 583/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 583/1471 [03:14\u003c04:28,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 584/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 584/1471 [03:15\u003c04:21,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 585/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 585/1471 [03:15\u003c04:19,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 586/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 586/1471 [03:15\u003c04:19,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 587/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 587/1471 [03:16\u003c04:20,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 588/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñâ      | 588/1471 [03:16\u003c04:15,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 589/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 589/1471 [03:16\u003c04:11,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 590/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 590/1471 [03:16\u003c04:12,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 591/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 591/1471 [03:17\u003c04:11,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 592/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 592/1471 [03:17\u003c04:07,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 593/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 593/1471 [03:17\u003c04:03,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 594/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 594/1471 [03:18\u003c04:05,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 595/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  40%|‚ñà‚ñà‚ñà‚ñà      | 595/1471 [03:18\u003c04:00,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 596/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 596/1471 [03:18\u003c03:59,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 597/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 597/1471 [03:18\u003c03:59,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 598/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 598/1471 [03:19\u003c04:00,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 599/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 599/1471 [03:19\u003c03:59,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 600/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 600/1471 [03:19\u003c03:58,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 601/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 601/1471 [03:19\u003c03:56,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 602/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 602/1471 [03:20\u003c03:59,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 603/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 603/1471 [03:20\u003c03:58,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 604/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 604/1471 [03:20\u003c03:55,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 605/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 605/1471 [03:21\u003c03:53,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 606/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà      | 606/1471 [03:21\u003c03:56,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 607/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 607/1471 [03:21\u003c03:58,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 608/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 608/1471 [03:21\u003c03:56,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 609/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 609/1471 [03:22\u003c04:04,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 610/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 610/1471 [03:22\u003c04:04,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 611/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 611/1471 [03:22\u003c04:02,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 612/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 612/1471 [03:23\u003c03:59,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 613/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 613/1471 [03:23\u003c04:30,  3.17it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 614/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 614/1471 [03:23\u003c04:56,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 615/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 615/1471 [03:24\u003c05:12,  2.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 616/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 616/1471 [03:24\u003c05:26,  2.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 617/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 617/1471 [03:25\u003c05:28,  2.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 618/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 618/1471 [03:25\u003c05:36,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 619/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 619/1471 [03:25\u003c05:43,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 620/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 620/1471 [03:26\u003c05:46,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 621/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 621/1471 [03:26\u003c05:55,  2.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 622/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 622/1471 [03:27\u003c05:23,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 623/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 623/1471 [03:27\u003c04:59,  2.83it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 624/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 624/1471 [03:27\u003c04:42,  2.99it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 625/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 625/1471 [03:27\u003c04:25,  3.18it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 626/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 626/1471 [03:28\u003c04:18,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 627/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 627/1471 [03:28\u003c04:09,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 628/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 628/1471 [03:28\u003c04:06,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 629/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 629/1471 [03:29\u003c04:02,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 630/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 630/1471 [03:29\u003c03:57,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 631/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 631/1471 [03:29\u003c03:55,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 632/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 632/1471 [03:29\u003c03:56,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 633/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 633/1471 [03:30\u003c03:53,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 634/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 634/1471 [03:30\u003c03:52,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 635/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 635/1471 [03:30\u003c03:51,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 636/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 636/1471 [03:30\u003c03:55,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 637/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 637/1471 [03:31\u003c03:53,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 638/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 638/1471 [03:31\u003c03:50,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 639/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 639/1471 [03:31\u003c03:52,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 640/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 640/1471 [03:32\u003c03:51,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 641/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 641/1471 [03:32\u003c03:52,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 642/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 642/1471 [03:32\u003c03:51,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 643/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 643/1471 [03:32\u003c03:51,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 644/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 644/1471 [03:33\u003c03:47,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 645/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 645/1471 [03:33\u003c03:44,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 646/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 646/1471 [03:33\u003c03:44,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 647/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 647/1471 [03:34\u003c03:47,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 648/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 648/1471 [03:34\u003c03:45,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 649/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 649/1471 [03:34\u003c03:45,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 650/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 650/1471 [03:34\u003c03:43,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 651/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 651/1471 [03:35\u003c03:44,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 652/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 652/1471 [03:35\u003c03:41,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 653/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 653/1471 [03:35\u003c03:42,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 654/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 654/1471 [03:35\u003c03:49,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 655/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 655/1471 [03:36\u003c03:49,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 656/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 656/1471 [03:36\u003c03:48,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 657/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 657/1471 [03:36\u003c03:52,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 658/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 658/1471 [03:37\u003c04:18,  3.14it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 659/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 659/1471 [03:37\u003c04:37,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 660/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 660/1471 [03:37\u003c04:47,  2.82it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 661/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 661/1471 [03:38\u003c04:58,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 662/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 662/1471 [03:38\u003c05:09,  2.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 663/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 663/1471 [03:39\u003c05:10,  2.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 664/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 664/1471 [03:39\u003c05:22,  2.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 665/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 665/1471 [03:40\u003c05:30,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 666/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 666/1471 [03:40\u003c05:37,  2.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 667/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 667/1471 [03:40\u003c05:04,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 668/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 668/1471 [03:41\u003c04:38,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 669/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 669/1471 [03:41\u003c04:18,  3.10it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 670/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 670/1471 [03:41\u003c04:10,  3.19it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 671/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 671/1471 [03:41\u003c04:01,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 672/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 672/1471 [03:42\u003c03:55,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 673/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 673/1471 [03:42\u003c03:51,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 674/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 674/1471 [03:42\u003c03:48,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 675/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 675/1471 [03:43\u003c03:45,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 676/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 676/1471 [03:43\u003c03:38,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 677/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 677/1471 [03:43\u003c03:41,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 678/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 678/1471 [03:43\u003c03:39,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 679/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 679/1471 [03:44\u003c03:37,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 680/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 680/1471 [03:44\u003c03:35,  3.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 681/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 681/1471 [03:44\u003c03:40,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 682/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 682/1471 [03:44\u003c03:39,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 683/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 683/1471 [03:45\u003c03:36,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 684/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 684/1471 [03:45\u003c03:37,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 685/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 685/1471 [03:45\u003c03:40,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 686/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 686/1471 [03:46\u003c03:40,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 687/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 687/1471 [03:46\u003c03:38,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 688/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 688/1471 [03:46\u003c03:36,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 689/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 689/1471 [03:46\u003c03:38,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 690/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 690/1471 [03:47\u003c03:40,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 691/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 691/1471 [03:47\u003c03:38,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 692/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 692/1471 [03:47\u003c03:40,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 693/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 693/1471 [03:48\u003c03:38,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 694/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 694/1471 [03:48\u003c03:35,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 695/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 695/1471 [03:48\u003c03:33,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 696/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 696/1471 [03:48\u003c03:39,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 697/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 697/1471 [03:49\u003c03:37,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 698/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 698/1471 [03:49\u003c03:37,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 699/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 699/1471 [03:49\u003c03:38,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 700/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 700/1471 [03:49\u003c03:39,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 701/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 701/1471 [03:50\u003c03:43,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 702/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 702/1471 [03:50\u003c03:46,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 703/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 703/1471 [03:51\u003c04:17,  2.98it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 704/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 704/1471 [03:51\u003c04:35,  2.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 705/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 705/1471 [03:51\u003c04:45,  2.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 706/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 706/1471 [03:52\u003c05:02,  2.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 707/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 707/1471 [03:52\u003c05:05,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 708/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 708/1471 [03:53\u003c05:05,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 709/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 709/1471 [03:53\u003c05:15,  2.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 710/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 710/1471 [03:53\u003c05:20,  2.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 711/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 711/1471 [03:54\u003c05:25,  2.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 712/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 712/1471 [03:54\u003c04:57,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 713/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 713/1471 [03:55\u003c04:33,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 714/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 714/1471 [03:55\u003c04:17,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 715/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 715/1471 [03:55\u003c04:04,  3.09it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 716/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 716/1471 [03:55\u003c03:55,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 717/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 717/1471 [03:56\u003c03:52,  3.24it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 718/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 718/1471 [03:56\u003c03:51,  3.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 719/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 719/1471 [03:56\u003c03:44,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 720/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 720/1471 [03:57\u003c03:40,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 721/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 721/1471 [03:57\u003c03:37,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 722/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 722/1471 [03:57\u003c03:38,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 723/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 723/1471 [03:57\u003c03:36,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 724/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 724/1471 [03:58\u003c03:31,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 725/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 725/1471 [03:58\u003c03:32,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 726/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 726/1471 [03:58\u003c03:30,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 727/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 727/1471 [03:59\u003c03:29,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 728/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 728/1471 [03:59\u003c03:27,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 729/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 729/1471 [03:59\u003c03:31,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 730/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 730/1471 [03:59\u003c03:28,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 731/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 731/1471 [04:00\u003c03:27,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 732/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 732/1471 [04:00\u003c03:25,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 733/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 733/1471 [04:00\u003c03:28,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 734/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 734/1471 [04:00\u003c03:27,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 735/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 735/1471 [04:01\u003c03:23,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 736/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 736/1471 [04:01\u003c03:21,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 737/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 737/1471 [04:01\u003c03:25,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 738/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 738/1471 [04:02\u003c03:24,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 739/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 739/1471 [04:02\u003c03:26,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 740/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 740/1471 [04:02\u003c03:27,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 741/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 741/1471 [04:02\u003c03:25,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 742/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 742/1471 [04:03\u003c03:23,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 743/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 743/1471 [04:03\u003c03:21,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 744/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 744/1471 [04:03\u003c03:23,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 745/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 745/1471 [04:04\u003c03:22,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 746/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 746/1471 [04:04\u003c03:21,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 747/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 747/1471 [04:04\u003c03:44,  3.22it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 748/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 748/1471 [04:05\u003c04:06,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 749/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 749/1471 [04:05\u003c04:16,  2.81it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 750/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 750/1471 [04:05\u003c04:32,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 751/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 751/1471 [04:06\u003c04:36,  2.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 752/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 752/1471 [04:06\u003c04:41,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 753/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 753/1471 [04:07\u003c04:51,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 754/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 754/1471 [04:07\u003c04:55,  2.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 755/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 755/1471 [04:08\u003c05:02,  2.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 756/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 756/1471 [04:08\u003c04:46,  2.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 757/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 757/1471 [04:08\u003c04:20,  2.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 758/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 758/1471 [04:08\u003c04:00,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 759/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 759/1471 [04:09\u003c03:49,  3.10it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 760/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 760/1471 [04:09\u003c03:39,  3.24it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 761/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 761/1471 [04:09\u003c03:35,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 762/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 762/1471 [04:10\u003c03:31,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 763/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 763/1471 [04:10\u003c03:28,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 764/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 764/1471 [04:10\u003c03:25,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 765/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 765/1471 [04:10\u003c03:23,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 766/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 766/1471 [04:11\u003c03:21,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 767/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 767/1471 [04:11\u003c03:22,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 768/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 768/1471 [04:11\u003c03:20,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 769/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 769/1471 [04:12\u003c03:19,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 770/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 770/1471 [04:12\u003c03:22,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 771/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 771/1471 [04:12\u003c03:21,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 772/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 772/1471 [04:12\u003c03:16,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 773/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 773/1471 [04:13\u003c03:14,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 774/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 774/1471 [04:13\u003c03:15,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 775/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 775/1471 [04:13\u003c03:12,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 776/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 776/1471 [04:14\u003c03:11,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 777/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 777/1471 [04:14\u003c03:11,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 778/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 778/1471 [04:14\u003c03:14,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 779/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 779/1471 [04:14\u003c03:15,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 780/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 780/1471 [04:15\u003c03:13,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 781/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 781/1471 [04:15\u003c03:12,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 782/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 782/1471 [04:15\u003c03:14,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 783/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 783/1471 [04:16\u003c03:14,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 784/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 784/1471 [04:16\u003c03:13,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 785/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 785/1471 [04:16\u003c03:15,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 786/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 786/1471 [04:16\u003c03:11,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 787/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 787/1471 [04:17\u003c03:09,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 788/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 788/1471 [04:17\u003c03:09,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 789/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 789/1471 [04:17\u003c03:10,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 790/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 790/1471 [04:17\u003c03:09,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 791/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 791/1471 [04:18\u003c03:07,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 792/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 792/1471 [04:18\u003c03:29,  3.24it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 793/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 793/1471 [04:19\u003c03:49,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 794/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 794/1471 [04:19\u003c04:00,  2.82it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 795/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 795/1471 [04:19\u003c04:09,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 796/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 796/1471 [04:20\u003c04:14,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 797/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 797/1471 [04:20\u003c04:18,  2.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 798/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 798/1471 [04:21\u003c04:22,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 799/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 799/1471 [04:21\u003c04:29,  2.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 800/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 800/1471 [04:21\u003c04:35,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 801/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 801/1471 [04:22\u003c04:29,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 802/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 802/1471 [04:22\u003c04:06,  2.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 803/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 803/1471 [04:22\u003c03:45,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 804/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 804/1471 [04:23\u003c03:35,  3.09it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 805/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 805/1471 [04:23\u003c03:26,  3.23it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 806/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 806/1471 [04:23\u003c03:21,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 807/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 807/1471 [04:23\u003c03:19,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 808/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 808/1471 [04:24\u003c03:19,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 809/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 809/1471 [04:24\u003c03:20,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 810/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 810/1471 [04:24\u003c03:18,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 811/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 811/1471 [04:25\u003c03:16,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 812/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 812/1471 [04:25\u003c03:12,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 813/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 813/1471 [04:25\u003c03:12,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 814/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 814/1471 [04:26\u003c03:12,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 815/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 815/1471 [04:26\u003c03:15,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 816/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 816/1471 [04:26\u003c03:13,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 817/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 817/1471 [04:26\u003c03:12,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 818/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 818/1471 [04:27\u003c03:15,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 819/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 819/1471 [04:27\u003c03:15,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 820/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 820/1471 [04:27\u003c03:12,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 821/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 821/1471 [04:28\u003c03:10,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 822/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 822/1471 [04:28\u003c03:08,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 823/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 823/1471 [04:28\u003c03:05,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 824/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 824/1471 [04:28\u003c03:04,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 825/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 825/1471 [04:29\u003c03:02,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 826/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 826/1471 [04:29\u003c03:05,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 827/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 827/1471 [04:29\u003c03:02,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 828/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 828/1471 [04:30\u003c03:01,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 829/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 829/1471 [04:30\u003c03:02,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 830/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 830/1471 [04:30\u003c03:02,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 831/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 831/1471 [04:30\u003c03:00,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 832/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 832/1471 [04:31\u003c02:59,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 833/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 833/1471 [04:31\u003c03:00,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 834/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 834/1471 [04:31\u003c02:58,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 835/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 835/1471 [04:32\u003c02:58,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 836/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 836/1471 [04:32\u003c03:16,  3.24it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 837/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 837/1471 [04:32\u003c03:37,  2.91it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 838/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 838/1471 [04:33\u003c03:48,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 839/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 839/1471 [04:33\u003c03:57,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 840/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 840/1471 [04:34\u003c03:58,  2.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 841/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 841/1471 [04:34\u003c03:59,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 842/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 842/1471 [04:34\u003c04:09,  2.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 843/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 843/1471 [04:35\u003c04:17,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 844/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 844/1471 [04:35\u003c04:30,  2.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 845/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 845/1471 [04:36\u003c04:13,  2.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 846/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 846/1471 [04:36\u003c03:50,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 847/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 847/1471 [04:36\u003c03:35,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 848/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 848/1471 [04:37\u003c03:32,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 849/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 849/1471 [04:37\u003c03:21,  3.09it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 850/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 850/1471 [04:37\u003c03:12,  3.23it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 851/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 851/1471 [04:37\u003c03:08,  3.28it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 852/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 852/1471 [04:38\u003c03:04,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 853/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 853/1471 [04:38\u003c02:58,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 854/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 854/1471 [04:38\u003c02:55,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 855/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 855/1471 [04:39\u003c02:56,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 856/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 856/1471 [04:39\u003c02:54,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 857/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 857/1471 [04:39\u003c02:53,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 858/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 858/1471 [04:39\u003c02:51,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 859/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 859/1471 [04:40\u003c02:52,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 860/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 860/1471 [04:40\u003c02:50,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 861/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 861/1471 [04:40\u003c02:49,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 862/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 862/1471 [04:40\u003c02:48,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 863/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 863/1471 [04:41\u003c02:50,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 864/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 864/1471 [04:41\u003c02:47,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 865/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 865/1471 [04:41\u003c02:47,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 866/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 866/1471 [04:42\u003c02:51,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 867/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 867/1471 [04:42\u003c02:50,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 868/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 868/1471 [04:42\u003c02:49,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 869/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 869/1471 [04:42\u003c02:54,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 870/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 870/1471 [04:43\u003c03:19,  3.01it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 871/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 871/1471 [04:43\u003c03:45,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 872/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 872/1471 [04:44\u003c03:51,  2.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 873/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 873/1471 [04:44\u003c03:37,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 874/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 874/1471 [04:44\u003c03:21,  2.96it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 875/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 875/1471 [04:45\u003c03:11,  3.12it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 876/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 876/1471 [04:45\u003c03:13,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 877/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 877/1471 [04:45\u003c03:05,  3.20it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 878/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 878/1471 [04:46\u003c03:03,  3.22it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 879/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 879/1471 [04:46\u003c03:24,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 880/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 880/1471 [04:46\u003c03:41,  2.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 881/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 881/1471 [04:47\u003c03:49,  2.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 882/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 882/1471 [04:47\u003c03:53,  2.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 883/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 883/1471 [04:48\u003c03:53,  2.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 884/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 884/1471 [04:48\u003c03:57,  2.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 885/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 885/1471 [04:49\u003c04:02,  2.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 886/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 886/1471 [04:49\u003c04:07,  2.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 887/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 887/1471 [04:49\u003c03:55,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 888/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 888/1471 [04:50\u003c03:34,  2.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 889/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 889/1471 [04:50\u003c03:17,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 890/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 890/1471 [04:50\u003c03:07,  3.10it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 891/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 891/1471 [04:50\u003c03:00,  3.22it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 892/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 892/1471 [04:51\u003c02:54,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 893/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 893/1471 [04:51\u003c02:49,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 894/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 894/1471 [04:51\u003c02:48,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 895/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 895/1471 [04:52\u003c02:48,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 896/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 896/1471 [04:52\u003c02:45,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 897/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 897/1471 [04:52\u003c02:44,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 898/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 898/1471 [04:52\u003c02:44,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 899/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 899/1471 [04:53\u003c02:42,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 900/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 900/1471 [04:53\u003c02:40,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 901/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 901/1471 [04:53\u003c02:39,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 902/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 902/1471 [04:54\u003c02:41,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 903/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 903/1471 [04:54\u003c02:41,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 904/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 904/1471 [04:54\u003c02:39,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 905/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 905/1471 [04:54\u003c02:40,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 906/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 906/1471 [04:55\u003c02:38,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 907/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 907/1471 [04:55\u003c02:35,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 908/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 908/1471 [04:55\u003c02:34,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 909/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 909/1471 [04:56\u003c02:36,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 910/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 910/1471 [04:56\u003c02:35,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 911/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 911/1471 [04:56\u003c02:34,  3.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 912/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 912/1471 [04:56\u003c02:33,  3.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 913/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 913/1471 [04:57\u003c02:37,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 914/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 914/1471 [04:57\u003c02:36,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 915/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 915/1471 [04:57\u003c02:35,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 916/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 916/1471 [04:57\u003c02:34,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 917/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 917/1471 [04:58\u003c02:36,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 918/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 918/1471 [04:58\u003c02:34,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 919/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 919/1471 [04:58\u003c02:32,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 920/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 920/1471 [04:59\u003c02:35,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 921/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 921/1471 [04:59\u003c02:32,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 922/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 922/1471 [04:59\u003c02:30,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 923/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 923/1471 [05:00\u003c02:48,  3.25it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 924/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 924/1471 [05:00\u003c03:07,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 925/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 925/1471 [05:00\u003c03:15,  2.79it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 926/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 926/1471 [05:01\u003c03:26,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 927/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 927/1471 [05:01\u003c03:31,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 928/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 928/1471 [05:02\u003c03:32,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 929/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 929/1471 [05:02\u003c03:40,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 930/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 930/1471 [05:02\u003c03:46,  2.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 931/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 931/1471 [05:03\u003c03:46,  2.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 932/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 932/1471 [05:03\u003c03:35,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 933/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 933/1471 [05:04\u003c03:16,  2.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 934/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 934/1471 [05:04\u003c02:59,  2.99it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 935/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 935/1471 [05:04\u003c02:53,  3.08it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 936/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 936/1471 [05:04\u003c02:45,  3.24it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 937/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 937/1471 [05:05\u003c02:40,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 938/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 938/1471 [05:05\u003c02:35,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 939/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 939/1471 [05:05\u003c02:35,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 940/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 940/1471 [05:05\u003c02:33,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 941/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 941/1471 [05:06\u003c02:30,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 942/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 942/1471 [05:06\u003c02:30,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 943/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 943/1471 [05:06\u003c02:31,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 944/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 944/1471 [05:07\u003c02:35,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 945/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 945/1471 [05:07\u003c02:34,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 946/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 946/1471 [05:07\u003c02:33,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 947/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 947/1471 [05:08\u003c02:31,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 948/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 948/1471 [05:08\u003c02:28,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 949/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 949/1471 [05:08\u003c02:28,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 950/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 950/1471 [05:08\u003c02:29,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 951/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 951/1471 [05:09\u003c02:27,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 952/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 952/1471 [05:09\u003c02:28,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 953/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 953/1471 [05:09\u003c02:28,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 954/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 954/1471 [05:09\u003c02:27,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 955/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 955/1471 [05:10\u003c02:25,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 956/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 956/1471 [05:10\u003c02:24,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 957/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 957/1471 [05:10\u003c02:26,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 958/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 958/1471 [05:11\u003c02:26,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 959/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 959/1471 [05:11\u003c02:28,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 960/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 960/1471 [05:11\u003c02:28,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 961/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 961/1471 [05:12\u003c02:31,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 962/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 962/1471 [05:12\u003c02:31,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 963/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 963/1471 [05:12\u003c02:30,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 964/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 964/1471 [05:12\u003c02:31,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 965/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 965/1471 [05:13\u003c02:28,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 966/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 966/1471 [05:13\u003c02:26,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 967/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 967/1471 [05:13\u003c02:36,  3.22it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 968/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 968/1471 [05:14\u003c02:51,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 969/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 969/1471 [05:14\u003c03:02,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 970/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 970/1471 [05:15\u003c03:10,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 971/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 971/1471 [05:15\u003c03:13,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 972/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 972/1471 [05:15\u003c03:16,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 973/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 973/1471 [05:16\u003c03:23,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 974/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 974/1471 [05:16\u003c03:29,  2.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 975/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 975/1471 [05:17\u003c03:31,  2.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 976/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 976/1471 [05:17\u003c03:23,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 977/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 977/1471 [05:17\u003c03:02,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 978/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 978/1471 [05:18\u003c02:47,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 979/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 979/1471 [05:18\u003c02:41,  3.05it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 980/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 980/1471 [05:18\u003c02:32,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 981/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 981/1471 [05:19\u003c02:28,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 982/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 982/1471 [05:19\u003c02:24,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 983/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 983/1471 [05:19\u003c02:24,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 984/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 984/1471 [05:19\u003c02:21,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 985/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 985/1471 [05:20\u003c02:20,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 986/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 986/1471 [05:20\u003c02:17,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 987/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 987/1471 [05:20\u003c02:18,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 988/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 988/1471 [05:21\u003c02:17,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 989/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 989/1471 [05:21\u003c02:14,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 990/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 990/1471 [05:21\u003c02:18,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 991/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 991/1471 [05:21\u003c02:15,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 992/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 992/1471 [05:22\u003c02:16,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 993/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 993/1471 [05:22\u003c02:15,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 994/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 994/1471 [05:22\u003c02:16,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 995/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 995/1471 [05:22\u003c02:15,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 996/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 996/1471 [05:23\u003c02:13,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 997/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 997/1471 [05:23\u003c02:13,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 998/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 998/1471 [05:23\u003c02:15,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 999/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 999/1471 [05:24\u003c02:13,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1000/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1000/1471 [05:24\u003c02:14,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1001/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1001/1471 [05:24\u003c02:16,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1002/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1002/1471 [05:24\u003c02:14,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1003/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1003/1471 [05:25\u003c02:12,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1004/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1004/1471 [05:25\u003c02:11,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1005/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1005/1471 [05:25\u003c02:13,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1006/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1006/1471 [05:26\u003c02:11,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1007/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1007/1471 [05:26\u003c02:11,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1008/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1008/1471 [05:26\u003c02:11,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1009/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1009/1471 [05:26\u003c02:11,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1010/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1010/1471 [05:27\u003c02:12,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1011/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1011/1471 [05:27\u003c02:13,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1012/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1012/1471 [05:27\u003c02:29,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1013/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1013/1471 [05:28\u003c02:41,  2.83it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1014/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1014/1471 [05:28\u003c02:47,  2.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1015/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1015/1471 [05:29\u003c02:51,  2.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1016/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1016/1471 [05:29\u003c02:52,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1017/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1017/1471 [05:29\u003c02:57,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1018/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1018/1471 [05:30\u003c03:04,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1019/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1019/1471 [05:30\u003c03:10,  2.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1020/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1020/1471 [05:31\u003c03:13,  2.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1021/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1021/1471 [05:31\u003c02:52,  2.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1022/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1022/1471 [05:31\u003c02:37,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1023/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1023/1471 [05:32\u003c02:27,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1024/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1024/1471 [05:32\u003c02:23,  3.12it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1025/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1025/1471 [05:32\u003c02:17,  3.24it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1026/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1026/1471 [05:33\u003c02:14,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1027/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1027/1471 [05:33\u003c02:12,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1028/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1028/1471 [05:33\u003c02:08,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1029/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1029/1471 [05:33\u003c02:07,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1030/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1030/1471 [05:34\u003c02:05,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1031/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1031/1471 [05:34\u003c02:05,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1032/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1032/1471 [05:34\u003c02:05,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1033/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1033/1471 [05:35\u003c02:04,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1034/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1034/1471 [05:35\u003c02:02,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1035/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1035/1471 [05:35\u003c02:03,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1036/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1036/1471 [05:35\u003c02:02,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1037/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1037/1471 [05:36\u003c02:02,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1038/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1038/1471 [05:36\u003c02:02,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1039/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1039/1471 [05:36\u003c02:02,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1040/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1040/1471 [05:36\u003c02:02,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1041/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1041/1471 [05:37\u003c02:06,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1042/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1042/1471 [05:37\u003c02:04,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1043/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1043/1471 [05:37\u003c02:02,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1044/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1044/1471 [05:38\u003c01:59,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1045/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1045/1471 [05:38\u003c01:58,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1046/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1046/1471 [05:38\u003c01:59,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1047/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1047/1471 [05:38\u003c02:00,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1048/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1048/1471 [05:39\u003c01:58,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1049/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1049/1471 [05:39\u003c01:59,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1050/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1050/1471 [05:39\u003c02:00,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1051/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1051/1471 [05:40\u003c01:58,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1052/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1052/1471 [05:40\u003c01:56,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1053/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1053/1471 [05:40\u003c01:58,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1054/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1054/1471 [05:40\u003c01:57,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1055/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1055/1471 [05:41\u003c01:55,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1056/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1056/1471 [05:41\u003c02:04,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1057/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1057/1471 [05:41\u003c02:19,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1058/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1058/1471 [05:42\u003c02:32,  2.71it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1059/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1059/1471 [05:42\u003c02:36,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1060/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1060/1471 [05:43\u003c02:39,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1061/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1061/1471 [05:43\u003c02:40,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1062/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1062/1471 [05:44\u003c02:41,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1063/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1063/1471 [05:44\u003c02:44,  2.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1064/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1064/1471 [05:44\u003c02:45,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1065/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1065/1471 [05:45\u003c02:42,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1066/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1066/1471 [05:45\u003c02:27,  2.74it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1067/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1067/1471 [05:45\u003c02:18,  2.92it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1068/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1068/1471 [05:46\u003c02:12,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1069/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1069/1471 [05:46\u003c02:05,  3.20it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1070/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1070/1471 [05:46\u003c02:02,  3.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1071/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1071/1471 [05:46\u003c01:58,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1072/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1072/1471 [05:47\u003c01:57,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1073/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1073/1471 [05:47\u003c01:54,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1074/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1074/1471 [05:47\u003c01:52,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1075/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1075/1471 [05:48\u003c01:52,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1076/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1076/1471 [05:48\u003c01:50,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1077/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1077/1471 [05:48\u003c01:49,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1078/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1078/1471 [05:48\u003c01:49,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1079/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1079/1471 [05:49\u003c01:49,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1080/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1080/1471 [05:49\u003c01:48,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1081/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1081/1471 [05:49\u003c01:47,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1082/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1082/1471 [05:50\u003c01:47,  3.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1083/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1083/1471 [05:50\u003c01:49,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1084/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1084/1471 [05:50\u003c01:47,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1085/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1085/1471 [05:50\u003c01:46,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1086/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1086/1471 [05:51\u003c01:45,  3.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1087/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1087/1471 [05:51\u003c01:47,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1088/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1088/1471 [05:51\u003c01:46,  3.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1089/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1089/1471 [05:51\u003c01:46,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1090/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1090/1471 [05:52\u003c01:47,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1091/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1091/1471 [05:52\u003c01:46,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1092/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1092/1471 [05:52\u003c01:46,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1093/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1093/1471 [05:53\u003c01:45,  3.59it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1094/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1094/1471 [05:53\u003c01:45,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1095/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1095/1471 [05:53\u003c01:44,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1096/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1096/1471 [05:53\u003c01:44,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1097/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1097/1471 [05:54\u003c01:46,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1098/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1098/1471 [05:54\u003c01:48,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1099/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1099/1471 [05:54\u003c01:47,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1100/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1100/1471 [05:55\u003c01:45,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1101/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1101/1471 [05:55\u003c01:56,  3.19it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1102/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1102/1471 [05:55\u003c02:06,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1103/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1103/1471 [05:56\u003c02:16,  2.70it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1104/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1104/1471 [05:56\u003c02:22,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1105/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1105/1471 [05:57\u003c02:23,  2.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1106/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1106/1471 [05:57\u003c02:26,  2.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1107/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1107/1471 [05:58\u003c02:27,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1108/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1108/1471 [05:58\u003c02:26,  2.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1109/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1109/1471 [05:58\u003c02:30,  2.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1110/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1110/1471 [05:59\u003c02:22,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1111/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1111/1471 [05:59\u003c02:08,  2.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1112/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1112/1471 [05:59\u003c01:58,  3.02it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1113/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1113/1471 [06:00\u003c01:53,  3.14it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1114/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1114/1471 [06:00\u003c01:49,  3.26it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1115/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1115/1471 [06:00\u003c01:46,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1116/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1116/1471 [06:00\u003c01:46,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1117/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1117/1471 [06:01\u003c01:45,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1118/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1118/1471 [06:01\u003c01:43,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1119/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1119/1471 [06:01\u003c01:41,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1120/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1120/1471 [06:02\u003c01:42,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1121/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1121/1471 [06:02\u003c01:42,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1122/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1122/1471 [06:02\u003c01:41,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1123/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1123/1471 [06:02\u003c01:41,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1124/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1124/1471 [06:03\u003c01:41,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1125/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1125/1471 [06:03\u003c01:39,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1126/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1126/1471 [06:03\u003c01:39,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1127/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1127/1471 [06:04\u003c01:39,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1128/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1128/1471 [06:04\u003c01:37,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1129/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1129/1471 [06:04\u003c01:36,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1130/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1130/1471 [06:04\u003c01:36,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1131/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1131/1471 [06:05\u003c01:36,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1132/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1132/1471 [06:05\u003c01:37,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1133/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1133/1471 [06:05\u003c01:38,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1134/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1134/1471 [06:06\u003c01:39,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1135/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1135/1471 [06:06\u003c01:38,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1136/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1136/1471 [06:06\u003c01:37,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1137/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1137/1471 [06:06\u003c01:36,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1138/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1138/1471 [06:07\u003c01:38,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1139/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1139/1471 [06:07\u003c01:40,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1140/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1140/1471 [06:07\u003c01:36,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1141/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1141/1471 [06:08\u003c01:35,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1142/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1142/1471 [06:08\u003c01:35,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1143/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1143/1471 [06:08\u003c01:33,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1144/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1144/1471 [06:08\u003c01:33,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1145/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1145/1471 [06:09\u003c01:42,  3.19it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1146/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1146/1471 [06:09\u003c01:49,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1147/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1147/1471 [06:10\u003c01:56,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1148/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1148/1471 [06:10\u003c02:02,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1149/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1149/1471 [06:10\u003c02:03,  2.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1150/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1150/1471 [06:11\u003c02:07,  2.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1151/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1151/1471 [06:11\u003c02:08,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1152/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1152/1471 [06:12\u003c02:16,  2.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1153/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1153/1471 [06:12\u003c02:19,  2.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1154/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1154/1471 [06:13\u003c02:09,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1155/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1155/1471 [06:13\u003c01:56,  2.72it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1156/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1156/1471 [06:13\u003c01:49,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1157/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1157/1471 [06:13\u003c01:41,  3.08it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1158/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1158/1471 [06:14\u003c01:36,  3.23it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1159/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1159/1471 [06:14\u003c01:33,  3.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1160/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1160/1471 [06:14\u003c01:32,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1161/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1161/1471 [06:15\u003c01:30,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1162/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1162/1471 [06:15\u003c01:29,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1163/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1163/1471 [06:15\u003c01:30,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1164/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1164/1471 [06:15\u003c01:29,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1165/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1165/1471 [06:16\u003c01:30,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1166/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1166/1471 [06:16\u003c01:29,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1167/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1167/1471 [06:16\u003c01:28,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1168/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1168/1471 [06:17\u003c01:27,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1169/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1169/1471 [06:17\u003c01:26,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1170/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1170/1471 [06:17\u003c01:26,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1171/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1171/1471 [06:17\u003c01:26,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1172/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1172/1471 [06:18\u003c01:25,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1173/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1173/1471 [06:18\u003c01:23,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1174/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1174/1471 [06:18\u003c01:24,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1175/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1175/1471 [06:19\u003c01:23,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1176/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1176/1471 [06:19\u003c01:22,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1177/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1177/1471 [06:19\u003c01:22,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1178/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1178/1471 [06:19\u003c01:23,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1179/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1179/1471 [06:20\u003c01:22,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1180/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1180/1471 [06:20\u003c01:21,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1181/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1181/1471 [06:20\u003c01:20,  3.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1182/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1182/1471 [06:21\u003c01:21,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1183/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1183/1471 [06:21\u003c01:20,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1184/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1184/1471 [06:21\u003c01:22,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1185/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1185/1471 [06:21\u003c01:22,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1186/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1186/1471 [06:22\u003c01:22,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1187/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1187/1471 [06:22\u003c01:20,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1188/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1188/1471 [06:22\u003c01:19,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1189/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1189/1471 [06:23\u003c01:24,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1190/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1190/1471 [06:23\u003c01:31,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1191/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1191/1471 [06:23\u003c01:37,  2.87it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1192/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1192/1471 [06:24\u003c01:41,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1193/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1193/1471 [06:24\u003c01:43,  2.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1194/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1194/1471 [06:25\u003c01:46,  2.60it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1195/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1195/1471 [06:25\u003c01:47,  2.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1196/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1196/1471 [06:25\u003c01:52,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1197/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1197/1471 [06:26\u003c01:53,  2.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1198/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1198/1471 [06:26\u003c01:53,  2.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1199/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1199/1471 [06:27\u003c01:45,  2.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1200/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1200/1471 [06:27\u003c01:37,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1201/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1201/1471 [06:27\u003c01:30,  2.97it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1202/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1202/1471 [06:27\u003c01:25,  3.15it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1203/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1203/1471 [06:28\u003c01:21,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1204/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1204/1471 [06:28\u003c01:20,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1205/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1205/1471 [06:28\u003c01:18,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1206/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1206/1471 [06:29\u003c01:16,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1207/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1207/1471 [06:29\u003c01:15,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1208/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1208/1471 [06:29\u003c01:15,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1209/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1209/1471 [06:29\u003c01:15,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1210/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1210/1471 [06:30\u003c01:15,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1211/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1211/1471 [06:30\u003c01:15,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1212/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1212/1471 [06:30\u003c01:14,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1213/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1213/1471 [06:31\u003c01:14,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1214/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1214/1471 [06:31\u003c01:13,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1215/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1215/1471 [06:31\u003c01:14,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1216/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1216/1471 [06:31\u003c01:13,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1217/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1217/1471 [06:32\u003c01:14,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1218/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1218/1471 [06:32\u003c01:13,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1219/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1219/1471 [06:32\u003c01:13,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1220/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1220/1471 [06:33\u003c01:13,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1221/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1221/1471 [06:33\u003c01:12,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1222/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1222/1471 [06:33\u003c01:12,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1223/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1223/1471 [06:34\u003c01:12,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1224/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1224/1471 [06:34\u003c01:11,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1225/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1225/1471 [06:34\u003c01:11,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1226/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1226/1471 [06:34\u003c01:11,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1227/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1227/1471 [06:35\u003c01:10,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1228/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1228/1471 [06:35\u003c01:09,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1229/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1229/1471 [06:35\u003c01:09,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1230/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1230/1471 [06:36\u003c01:10,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1231/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1231/1471 [06:36\u003c01:10,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1232/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1232/1471 [06:36\u003c01:09,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1233/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1233/1471 [06:36\u003c01:09,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1234/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1234/1471 [06:37\u003c01:17,  3.06it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1235/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1235/1471 [06:37\u003c01:22,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1236/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1236/1471 [06:38\u003c01:27,  2.69it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1237/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1237/1471 [06:38\u003c01:29,  2.62it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1238/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1238/1471 [06:39\u003c01:32,  2.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1239/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1239/1471 [06:39\u003c01:33,  2.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1240/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1240/1471 [06:39\u003c01:33,  2.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1241/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1241/1471 [06:40\u003c01:36,  2.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1242/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1242/1471 [06:40\u003c01:35,  2.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1243/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1243/1471 [06:41\u003c01:30,  2.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1244/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1244/1471 [06:41\u003c01:22,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1245/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1245/1471 [06:41\u003c01:17,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1246/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1246/1471 [06:41\u003c01:13,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1247/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1247/1471 [06:42\u003c01:11,  3.15it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1248/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1248/1471 [06:42\u003c01:09,  3.22it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1249/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1249/1471 [06:42\u003c01:07,  3.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1250/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1250/1471 [06:43\u003c01:05,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1251/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1251/1471 [06:43\u003c01:05,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1252/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1252/1471 [06:43\u003c01:03,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1253/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1253/1471 [06:43\u003c01:02,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1254/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1254/1471 [06:44\u003c01:03,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1255/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1255/1471 [06:44\u003c01:02,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1256/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1256/1471 [06:44\u003c01:01,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1257/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1257/1471 [06:45\u003c01:00,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1258/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1258/1471 [06:45\u003c01:01,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1259/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1259/1471 [06:45\u003c01:00,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1260/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1260/1471 [06:45\u003c01:00,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1261/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1261/1471 [06:46\u003c01:00,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1262/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1262/1471 [06:46\u003c01:01,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1263/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1263/1471 [06:46\u003c01:01,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1264/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1264/1471 [06:47\u003c01:01,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1265/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1265/1471 [06:47\u003c01:03,  3.23it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1266/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1266/1471 [06:47\u003c01:01,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1267/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1267/1471 [06:48\u003c00:59,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1268/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 1268/1471 [06:48\u003c00:59,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1269/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1269/1471 [06:48\u003c00:59,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1270/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1270/1471 [06:48\u003c00:57,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1271/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1271/1471 [06:49\u003c00:56,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1272/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1272/1471 [06:49\u003c00:56,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1273/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1273/1471 [06:49\u003c00:55,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1274/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1274/1471 [06:50\u003c00:55,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1275/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1275/1471 [06:50\u003c00:55,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1276/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1276/1471 [06:50\u003c00:55,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1277/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1277/1471 [06:50\u003c00:54,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1278/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1278/1471 [06:51\u003c01:01,  3.14it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1279/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1279/1471 [06:51\u003c01:07,  2.84it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1280/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1280/1471 [06:52\u003c01:09,  2.75it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1281/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1281/1471 [06:52\u003c01:11,  2.66it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1282/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1282/1471 [06:52\u003c01:13,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1283/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1283/1471 [06:53\u003c01:14,  2.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1284/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1284/1471 [06:53\u003c01:14,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1285/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1285/1471 [06:54\u003c01:16,  2.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1286/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1286/1471 [06:54\u003c01:18,  2.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1287/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1287/1471 [06:54\u003c01:14,  2.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1288/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1288/1471 [06:55\u003c01:07,  2.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1289/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1289/1471 [06:55\u003c01:01,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1290/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1290/1471 [06:55\u003c00:58,  3.11it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1291/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1291/1471 [06:56\u003c00:56,  3.17it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1292/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1292/1471 [06:56\u003c00:55,  3.23it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1293/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1293/1471 [06:56\u003c00:53,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1294/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1294/1471 [06:56\u003c00:52,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1295/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1295/1471 [06:57\u003c00:52,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1296/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1296/1471 [06:57\u003c00:51,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1297/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1297/1471 [06:57\u003c00:50,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1298/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1298/1471 [06:58\u003c00:50,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1299/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1299/1471 [06:58\u003c00:48,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1300/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1300/1471 [06:58\u003c00:48,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1301/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1301/1471 [06:58\u003c00:47,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1302/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1302/1471 [06:59\u003c00:48,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1303/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1303/1471 [06:59\u003c00:47,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1304/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1304/1471 [06:59\u003c00:47,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1305/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1305/1471 [07:00\u003c00:47,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1306/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1306/1471 [07:00\u003c00:46,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1307/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1307/1471 [07:00\u003c00:46,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1308/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1308/1471 [07:00\u003c00:45,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1309/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1309/1471 [07:01\u003c00:46,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1310/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1310/1471 [07:01\u003c00:45,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1311/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1311/1471 [07:01\u003c00:45,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1312/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1312/1471 [07:02\u003c00:44,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1313/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1313/1471 [07:02\u003c00:45,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1314/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1314/1471 [07:02\u003c00:44,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1315/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1315/1471 [07:02\u003c00:43,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1316/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1316/1471 [07:03\u003c00:44,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1317/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1317/1471 [07:03\u003c00:43,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1318/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1318/1471 [07:03\u003c00:42,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1319/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1319/1471 [07:04\u003c00:42,  3.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1320/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1320/1471 [07:04\u003c00:43,  3.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1321/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1321/1471 [07:04\u003c00:43,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1322/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1322/1471 [07:04\u003c00:45,  3.27it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1323/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1323/1471 [07:05\u003c00:50,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1324/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1324/1471 [07:05\u003c00:52,  2.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1325/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1325/1471 [07:06\u003c00:54,  2.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1326/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1326/1471 [07:06\u003c00:56,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1327/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1327/1471 [07:07\u003c00:56,  2.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1328/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1328/1471 [07:07\u003c00:58,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1329/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1329/1471 [07:07\u003c00:59,  2.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1330/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1330/1471 [07:08\u003c00:59,  2.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1331/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1331/1471 [07:08\u003c00:59,  2.35it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1332/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1332/1471 [07:09\u003c00:54,  2.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1333/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1333/1471 [07:09\u003c00:50,  2.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1334/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1334/1471 [07:09\u003c00:47,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1335/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1335/1471 [07:10\u003c00:44,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1336/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1336/1471 [07:10\u003c00:42,  3.19it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1337/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1337/1471 [07:10\u003c00:40,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1338/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1338/1471 [07:10\u003c00:39,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1339/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1339/1471 [07:11\u003c00:38,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1340/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1340/1471 [07:11\u003c00:38,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1341/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1341/1471 [07:11\u003c00:37,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1342/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1342/1471 [07:12\u003c00:37,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1343/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1343/1471 [07:12\u003c00:37,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1344/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1344/1471 [07:12\u003c00:37,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1345/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1345/1471 [07:12\u003c00:37,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1346/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1346/1471 [07:13\u003c00:37,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1347/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1347/1471 [07:13\u003c00:36,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1348/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1348/1471 [07:13\u003c00:36,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1349/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1349/1471 [07:14\u003c00:36,  3.39it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1350/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1350/1471 [07:14\u003c00:35,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1351/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1351/1471 [07:14\u003c00:34,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1352/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1352/1471 [07:14\u003c00:34,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1353/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1353/1471 [07:15\u003c00:34,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1354/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1354/1471 [07:15\u003c00:33,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1355/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1355/1471 [07:15\u003c00:33,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1356/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1356/1471 [07:16\u003c00:34,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1357/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1357/1471 [07:16\u003c00:33,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1358/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1358/1471 [07:16\u003c00:32,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1359/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1359/1471 [07:16\u003c00:32,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1360/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 1360/1471 [07:17\u003c00:32,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1361/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1361/1471 [07:17\u003c00:31,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1362/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1362/1471 [07:17\u003c00:31,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1363/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1363/1471 [07:18\u003c00:30,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1364/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1364/1471 [07:18\u003c00:30,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1365/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1365/1471 [07:18\u003c00:29,  3.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1366/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1366/1471 [07:18\u003c00:30,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1367/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1367/1471 [07:19\u003c00:34,  3.05it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1368/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1368/1471 [07:19\u003c00:36,  2.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1369/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1369/1471 [07:20\u003c00:38,  2.68it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1370/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1370/1471 [07:20\u003c00:38,  2.64it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1371/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1371/1471 [07:21\u003c00:37,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1372/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1372/1471 [07:21\u003c00:38,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1373/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1373/1471 [07:21\u003c00:39,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1374/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1374/1471 [07:22\u003c00:39,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1375/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1375/1471 [07:22\u003c00:40,  2.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1376/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1376/1471 [07:23\u003c00:37,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1377/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1377/1471 [07:23\u003c00:33,  2.78it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1378/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1378/1471 [07:23\u003c00:31,  2.94it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1379/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1379/1471 [07:23\u003c00:30,  3.07it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1380/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1380/1471 [07:24\u003c00:28,  3.16it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1381/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1381/1471 [07:24\u003c00:28,  3.12it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1382/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1382/1471 [07:24\u003c00:27,  3.20it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1383/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1383/1471 [07:25\u003c00:26,  3.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1384/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1384/1471 [07:25\u003c00:25,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1385/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1385/1471 [07:25\u003c00:25,  3.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1386/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1386/1471 [07:26\u003c00:25,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1387/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1387/1471 [07:26\u003c00:25,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1388/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1388/1471 [07:26\u003c00:25,  3.29it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1389/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1389/1471 [07:26\u003c00:24,  3.32it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1390/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1390/1471 [07:27\u003c00:24,  3.33it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1391/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1391/1471 [07:27\u003c00:23,  3.36it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1392/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1392/1471 [07:27\u003c00:23,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1393/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1393/1471 [07:28\u003c00:23,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1394/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1394/1471 [07:28\u003c00:22,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1395/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1395/1471 [07:28\u003c00:22,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1396/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1396/1471 [07:28\u003c00:21,  3.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1397/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1397/1471 [07:29\u003c00:21,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1398/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1398/1471 [07:29\u003c00:20,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1399/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1399/1471 [07:29\u003c00:20,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1400/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1400/1471 [07:30\u003c00:20,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1401/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1401/1471 [07:30\u003c00:19,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1402/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1402/1471 [07:30\u003c00:19,  3.55it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1403/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1403/1471 [07:30\u003c00:19,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1404/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1404/1471 [07:31\u003c00:18,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1405/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1405/1471 [07:31\u003c00:18,  3.52it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1406/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1406/1471 [07:31\u003c00:18,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1407/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1407/1471 [07:32\u003c00:18,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1408/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1408/1471 [07:32\u003c00:18,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1409/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1409/1471 [07:32\u003c00:17,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1410/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1410/1471 [07:33\u003c00:18,  3.30it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1411/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1411/1471 [07:33\u003c00:20,  2.93it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1412/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1412/1471 [07:33\u003c00:21,  2.77it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1413/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1413/1471 [07:34\u003c00:21,  2.65it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1414/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1414/1471 [07:34\u003c00:22,  2.58it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1415/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1415/1471 [07:35\u003c00:22,  2.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1416/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1416/1471 [07:35\u003c00:21,  2.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1417/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1417/1471 [07:35\u003c00:22,  2.42it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1418/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1418/1471 [07:36\u003c00:22,  2.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1419/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1419/1471 [07:36\u003c00:22,  2.31it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1420/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1420/1471 [07:37\u003c00:20,  2.51it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1421/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1421/1471 [07:37\u003c00:18,  2.73it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1422/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1422/1471 [07:37\u003c00:17,  2.86it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1423/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1423/1471 [07:38\u003c00:15,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1424/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1424/1471 [07:38\u003c00:14,  3.14it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1425/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1425/1471 [07:38\u003c00:14,  3.25it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1426/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1426/1471 [07:38\u003c00:13,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1427/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1427/1471 [07:39\u003c00:13,  3.37it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1428/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1428/1471 [07:39\u003c00:12,  3.38it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1429/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1429/1471 [07:39\u003c00:12,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1430/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1430/1471 [07:40\u003c00:11,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1431/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1431/1471 [07:40\u003c00:11,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1432/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1432/1471 [07:40\u003c00:11,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1433/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1433/1471 [07:40\u003c00:10,  3.53it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1434/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1434/1471 [07:41\u003c00:10,  3.56it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1435/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1435/1471 [07:41\u003c00:10,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1436/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1436/1471 [07:41\u003c00:10,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1437/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1437/1471 [07:42\u003c00:09,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1438/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1438/1471 [07:42\u003c00:09,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1439/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1439/1471 [07:42\u003c00:09,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1440/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1440/1471 [07:42\u003c00:08,  3.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1441/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1441/1471 [07:43\u003c00:08,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1442/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1442/1471 [07:43\u003c00:08,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1443/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1443/1471 [07:43\u003c00:08,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1444/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1444/1471 [07:44\u003c00:07,  3.48it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1445/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1445/1471 [07:44\u003c00:07,  3.49it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1446/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1446/1471 [07:44\u003c00:07,  3.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1447/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1447/1471 [07:44\u003c00:06,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1448/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1448/1471 [07:45\u003c00:06,  3.47it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1449/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1449/1471 [07:45\u003c00:06,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1450/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1450/1471 [07:45\u003c00:06,  3.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1451/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1451/1471 [07:46\u003c00:05,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1452/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 1452/1471 [07:46\u003c00:05,  3.46it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1453/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1453/1471 [07:46\u003c00:05,  3.40it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1454/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1454/1471 [07:47\u003c00:05,  3.21it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1455/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1455/1471 [07:47\u003c00:05,  2.95it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1456/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1456/1471 [07:47\u003c00:05,  2.80it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1457/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1457/1471 [07:48\u003c00:05,  2.67it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1458/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1458/1471 [07:48\u003c00:04,  2.63it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1459/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1459/1471 [07:49\u003c00:04,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1460/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1460/1471 [07:49\u003c00:04,  2.54it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1461/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1461/1471 [07:49\u003c00:04,  2.45it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1462/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1462/1471 [07:50\u003c00:03,  2.43it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1463/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1463/1471 [07:50\u003c00:03,  2.44it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1464/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1464/1471 [07:51\u003c00:02,  2.61it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1465/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1465/1471 [07:51\u003c00:02,  2.85it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1466/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1466/1471 [07:51\u003c00:01,  3.03it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1467/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1467/1471 [07:51\u003c00:01,  3.16it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1468/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1468/1471 [07:52\u003c00:00,  3.23it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1469/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1469/1471 [07:52\u003c00:00,  3.34it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1470/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["\rProcessing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1470/1471 [07:52\u003c00:00,  3.41it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","üîπ Processing image 1471/1471\n","  Image shape after preprocessing: (75, 75, 3)\n","  Input keys: ['pixel_values']\n","  Input tensor shape: torch.Size([1, 3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["Processing images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1471/1471 [07:53\u003c00:00,  3.11it/s]"]},{"name":"stdout","output_type":"stream","text":["  Outputs type: \u003cclass 'transformers.models.swin.modeling_swin.SwinModelOutput'\u003e\n","  Using pooler_output, shape: (1, 768)\n","\n","All features stacked, shape: (1471, 768)\n","All labels stacked, shape: (1471,)\n","Swin feature extraction complete! Saved to masked_training_df_swin_features.pkl\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Call extract features\n","get_embedding_swin(masked_training_df, 'band_1', 'band_2', processor, model, device, \"masked_training_df\")\n","get_embedding_swin(ssf_training_df, 'band_1', 'band_2', processor, model, device, \"ssf_training_df\")\n","get_embedding_swin(zoom_training_df, 'band_1', 'band_2', processor, model, device, \"zoom_training_df\")"]},{"cell_type":"markdown","metadata":{"id":"D2xP2xSS3tfX"},"source":["Use a plain timm Swin Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_1Sv3yo3wjz"},"outputs":[],"source":["device_swin_timm = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_swin_timm = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True)\n","model_swin_timm.eval()\n","model_swin_timm.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5iuFNS94Qu6"},"outputs":[],"source":["get_embedding(loader_masked, device=device_swin_timm)\n","get_embedding(loader_ssf, device=device_swin_timm)\n","get_embedding(loader_zoom, device=device_swin_timm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VAZb4_574kWo"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02bc7bea1e2b4216b8c989222fe18995":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_65a1406feba444548c22c76c0871560d"}},"237f6f5a9f02451193a7888f5ddce27b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_870ce613eec0448c85d26d21487ba5cb","placeholder":"‚Äã","style":"IPY_MODEL_f435ca9ce77e4dbeb62baca26e2ba35b","value":"model.safetensors:‚Äá100%"}},"44da250988c140d2ba7d6d55a3d3244c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef0488fc2f604f19a43de9b1c9897985","max":346284714,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fde8c23f648d4aacad04ba2af9343fb3","value":346284714}},"623ec52ce57247a79445a4151c751809":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ded473a8f14c96ad472ceb08348f0f","placeholder":"‚Äã","style":"IPY_MODEL_accbbb86011041cc98dc6f57365dda5c","value":"‚Äá346M/346M‚Äá[00:08\u0026lt;00:00,‚Äá72.4MB/s]"}},"65a1406feba444548c22c76c0871560d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"870ce613eec0448c85d26d21487ba5cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95e9c8f0dd4c46eb8905e7d2c3782412":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"982f2c4e64b2494b84122de9d7c14dee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_237f6f5a9f02451193a7888f5ddce27b","IPY_MODEL_44da250988c140d2ba7d6d55a3d3244c","IPY_MODEL_623ec52ce57247a79445a4151c751809"],"layout":"IPY_MODEL_95e9c8f0dd4c46eb8905e7d2c3782412"}},"a0ded473a8f14c96ad472ceb08348f0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"accbbb86011041cc98dc6f57365dda5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef0488fc2f604f19a43de9b1c9897985":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f435ca9ce77e4dbeb62baca26e2ba35b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fde8c23f648d4aacad04ba2af9343fb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}