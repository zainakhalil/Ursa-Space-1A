{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqXlr9ziw8P3BD5zITyQDC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The notebook mounts Google Drive, reads train.json from “Ursa Space 1A,” and converts each sample’s band_1 and band_2 lists into 75×75 float images, stacked as a two-channel tensor [N, 2, 75, 75]. Labels (is_iceberg) and incidence angles are loaded; angles are imputed with the train-set median for missing values and optionally z-normalized. A stratified 80/20 split creates train/validation indices. Per-channel statistics (mean, std) are computed on the train tensor and used to z-normalize both splits; all arrays plus angles are saved to a compact artifact sar_fullres_splits.npz for reproducibility. The model IcebergVesselCNN is a compact convolutional trunk (Conv-ReLU-MaxPool blocks → AdaptiveAvgPool to a 128-d feature) with an angle-fusion head that concatenates the (optionally normalized) angle to the pooled feature before a small MLP classifier. Training uses AdamW with cross-entropy loss, optional mixed precision (AMP), light SAR-appropriate augmentations (random horizontal/vertical flips and 90° rotations), and a ReduceLROnPlateau scheduler keyed to validation AUC, with early stopping based on AUC to prevent overfitting. After each epoch, accuracy and AUC are computed on the validation set and the best checkpoint is stored as best_iceberg_vessel_cnn.pt. Final reporting includes accuracy, ROC-AUC, and a confusion matrix (e.g., TN/FP/FN/TP), confirming strong performance on the iceberg-vs-vessel classification task."],"metadata":{"id":"vQhyZrb2TP_-"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qk-BQ8P7KT2s","executionInfo":{"status":"ok","timestamp":1760980285029,"user_tz":300,"elapsed":574,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"30f950bf-a27a-4e6b-9078-0ec16f5c3a48"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","%cd /content/Ursa-Space-1A\n","\n","os.environ[\"GITHUB_TOKEN\"] = userdata.get(\"GITHUB_TOKEN\")\n","\n","# set the push URL to use your token (do this once per session)\n","!git remote set-url --push origin https://x-access-token:${GITHUB_TOKEN}@github.com/zainakhalil/Ursa-Space-1A.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVT66QBRkBeh","executionInfo":{"status":"ok","timestamp":1760980294624,"user_tz":300,"elapsed":422,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"ae06d071-87c3-4135-ed46-4296e756a9ba"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Ursa-Space-1A\n"]}]},{"cell_type":"code","source":["%cd /content/Ursa-Space-1A\n","!git add -A\n","!git commit -m \"update\"\n","!git push origin HEAD:main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYp9pYb8kDvc","executionInfo":{"status":"ok","timestamp":1760981798673,"user_tz":300,"elapsed":1000,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"bcba54f4-a78f-42a9-dd64-02f0a5ba7ea0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Ursa-Space-1A\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","nothing to commit, working tree clean\n","Everything up-to-date\n"]}]},{"cell_type":"code","source":["# 0) Be in the repo\n","%cd /content/Ursa-Space-1A\n","!git rev-parse --is-inside-work-tree\n","\n","# 1) See what Git sees\n","!git status\n","\n","# 2) List files here (confirm your notebook is actually in this folder)\n","!ls -lah\n","\n","# 3) If your notebook is elsewhere, copy it in (pick ONE that matches your setup):\n","\n","# From /content (typical Colab default):\n","# !cp \"/content/YourNotebookName.ipynb\" .\n","\n","# From Drive (example path):\n","# !cp \"/content/drive/MyDrive/Colab Notebooks/YourNotebookName.ipynb\" .\n","\n","# 4) Check if .gitignore is hiding notebooks\n","!grep -n \"\\.ipynb\" .gitignore || echo \"No .ipynb rules in .gitignore\"\n","\n","# If it prints a line with *.ipynb, remove/comment that line or rename the file to be tracked.\n","\n","# 5) Stage, commit, push (minimal)\n","!git add -A\n","!git status\n","!git commit -m \"Update notebook and outputs\" || echo \"No changes to commit\"\n","!git push origin HEAD:main 2>/dev/null || git push origin HEAD:master\n"],"metadata":{"id":"XZPxh_Ztsplf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","!ls \"/content/drive/MyDrive/Ursa Space 1A/train.json\"\n","df = pd.read_json('/content/drive/MyDrive/Ursa Space 1A/train.json')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2tnLJgYKaVn","executionInfo":{"status":"ok","timestamp":1760980306019,"user_tz":300,"elapsed":9216,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"2b966a1b-75c8-4851-e34b-0e07491d4c3d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/drive/MyDrive/Ursa Space 1A/train.json'\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZim7rvJJdUr","executionInfo":{"status":"ok","timestamp":1760980323470,"user_tz":300,"elapsed":17446,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"c2b83624-79a0-43a9-f0af-061737066f39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1283, 2, 75, 75), (321, 2, 75, 75))"]},"metadata":{},"execution_count":3}],"source":["import math, numpy as np, pandas as pd\n","from sklearn.model_selection import train_test_split\n","from pathlib import Path\n","\n","DATA_PATH = Path(\"/content/drive/MyDrive/Ursa Space 1A/train.json\")\n","ART_DIR   = Path(\"/content/artifacts\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n","\n","df = pd.read_json(DATA_PATH)\n","\n","def to_img(seq):\n","    a = np.array(seq, dtype=np.float32)\n","    side = int(round(math.sqrt(a.size)))\n","    assert side*side == a.size, f\"Non-square band length {a.size}\"\n","    return a.reshape(side, side)\n","\n","# [N, 2, H, W]  (two channels: band_1, band_2)\n","X = np.stack([np.stack([to_img(b1), to_img(b2)], axis=0)\n","              for b1, b2 in zip(df[\"band_1\"], df[\"band_2\"])], axis=0)\n","y = df[\"is_iceberg\"].astype(np.int64).to_numpy()\n","inc_angle = pd.to_numeric(df[\"inc_angle\"], errors=\"coerce\").to_numpy()\n","\n","# stratified 80/20 split\n","idx = np.arange(len(y))\n","idx_tr, idx_va = train_test_split(idx, test_size=0.2, random_state=1234, stratify=y)\n","X_tr, X_va = X[idx_tr], X[idx_va]\n","y_tr, y_va = y[idx_tr], y[idx_va]\n","ang_tr, ang_va = inc_angle[idx_tr], inc_angle[idx_va]\n","\n","# per-channel z-normalization using train stats\n","m = X_tr.mean(axis=(0,2,3), keepdims=True)\n","s = X_tr.std(axis=(0,2,3), keepdims=True) + 1e-6\n","X_tr = (X_tr - m)/s\n","X_va = (X_va - m)/s\n","\n","np.savez_compressed(ART_DIR / \"sar_fullres_splits.npz\",\n","    X_train=X_tr, y_train=y_tr, X_val=X_va, y_val=y_va,\n","    means=m.squeeze(), stds=s.squeeze(), side=int(X.shape[-1]),\n","    inc_angle_train=ang_tr, inc_angle_val=ang_va\n",")\n","X_tr.shape, X_va.shape\n"]},{"cell_type":"code","source":["import numpy as np, torch, torch.nn as nn, torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from pathlib import Path\n","\n","data = np.load(\"/content/artifacts/sar_fullres_splits.npz\")\n","Xtr, Xva = data[\"X_train\"], data[\"X_val\"]\n","ytr, yva = data[\"y_train\"], data[\"y_val\"]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n","Xva_t = torch.tensor(Xva, dtype=torch.float32)\n","ytr_t = torch.tensor(ytr, dtype=torch.long)\n","yva_t = torch.tensor(yva, dtype=torch.long)\n","\n","train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=64, shuffle=True)\n","val_loader   = DataLoader(TensorDataset(Xva_t, yva_t), batch_size=128, shuffle=False)\n","\n","class SmallCNN(nn.Module):\n","    def __init__(self, in_ch=2, num_classes=2):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(in_ch, 16, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d(1),\n","        )\n","        self.fc = nn.Linear(64, num_classes)\n","    def forward(self, x):\n","        x = self.net(x).view(x.size(0), -1)\n","        return self.fc(x)\n","\n","model = SmallCNN().to(device)\n","opt = optim.Adam(model.parameters(), lr=1e-3)\n","crit = nn.CrossEntropyLoss()\n","\n","def run_epoch(dl, train=True):\n","    model.train() if train else model.eval()\n","    tot=correct=loss_sum=0\n","    for xb,yb in dl:\n","        xb,yb = xb.to(device), yb.to(device)\n","        if train: opt.zero_grad()\n","        with torch.set_grad_enabled(train):\n","            logits = model(xb); loss = crit(logits, yb)\n","            if train: loss.backward(); opt.step()\n","        pred = logits.argmax(1)\n","        correct += (pred==yb).sum().item()\n","        tot += yb.numel(); loss_sum += loss.item()*yb.size(0)\n","    return loss_sum/tot, correct/tot\n","\n","for ep in range(8):\n","    tr_loss,tr_acc = run_epoch(train_loader, True)\n","    va_loss,va_acc = run_epoch(val_loader, False)\n","    print(f\"Epoch {ep+1}: tr {tr_loss:.4f}/{tr_acc:.3f} | va {va_loss:.4f}/{va_acc:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNhSmZRTMYwK","executionInfo":{"status":"ok","timestamp":1760980380462,"user_tz":300,"elapsed":56985,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"f42cdbfd-8708-42cd-e56c-52d1ef9270d5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: tr 0.6639/0.567 | va 0.6381/0.623\n","Epoch 2: tr 0.6531/0.610 | va 0.6387/0.611\n","Epoch 3: tr 0.6456/0.627 | va 0.6285/0.660\n","Epoch 4: tr 0.6366/0.641 | va 0.6360/0.639\n","Epoch 5: tr 0.6320/0.659 | va 0.6081/0.670\n","Epoch 6: tr 0.6229/0.652 | va 0.5872/0.695\n","Epoch 7: tr 0.6014/0.672 | va 0.5672/0.688\n","Epoch 8: tr 0.5921/0.661 | va 0.5733/0.713\n"]}]},{"cell_type":"code","source":["# ==================================\n","# Config Small CNN + Small Utilities\n","# ==================================\n","from dataclasses import dataclass\n","from pathlib import Path\n","import numpy as np\n","import torch, torch.nn as nn, torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n","\n","@dataclass\n","class Config:\n","    # Paths\n","    npz_path: str = \"/content/artifacts/sar_fullres_splits.npz\"\n","    save_dir: str = \"/content/artifacts\"\n","    # Training\n","    batch_size: int = 64\n","    epochs: int = 20\n","    lr: float = 1e-3\n","    weight_decay: float = 1e-4\n","    # Early stopping / LR schedule\n","    patience: int = 5          # epochs with no AUC improvement to stop\n","    lr_patience: int = 2       # plateaus before LR reduce\n","    lr_gamma: float = 0.5\n","    # Augmentations\n","    hflip_p: float = 0.5\n","    vflip_p: float = 0.5\n","    rot90_p: float = 0.5       # random k*90° rotation\n","    # Incidence angle fusion\n","    use_angle: bool = True     # set False to ignore angle\n","    norm_angle: bool = True    # z-norm angle using train stats\n","\n","cfg = Config()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","Path(cfg.save_dir).mkdir(parents=True, exist_ok=True)\n","print(\"Using device:\", device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNQMvn5KOT98","executionInfo":{"status":"ok","timestamp":1760980380499,"user_tz":300,"elapsed":23,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"f96684a1-b682-4a52-d379-e432c5a2aef7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# ===========================\n","# Load NPZ + Prepare Tensors\n","# ===========================\n","D = np.load(cfg.npz_path)\n","\n","Xtr = D[\"X_train\"]          # [N, 2, 75, 75]\n","Xva = D[\"X_val\"]\n","ytr = D[\"y_train\"].astype(np.int64)\n","yva = D[\"y_val\"].astype(np.int64)\n","\n","# Angle arrays can have NaN; we'll impute with train median, then optionally z-norm\n","if cfg.use_angle:\n","    ang_tr = D[\"inc_angle_train\"].astype(np.float32)\n","    ang_va = D[\"inc_angle_val\"].astype(np.float32)\n","\n","    # Impute missing angles with train median (robust/simple)\n","    train_med = np.nanmedian(ang_tr)\n","    ang_tr = np.where(np.isfinite(ang_tr), ang_tr, train_med)\n","    ang_va = np.where(np.isfinite(ang_va), ang_va, train_med)\n","\n","    if cfg.norm_angle:\n","        mu, sd = ang_tr.mean(), ang_tr.std() + 1e-6\n","        ang_tr = (ang_tr - mu) / sd\n","        ang_va = (ang_va - mu) / sd\n","else:\n","    # If angle is disabled, create zeros so the loaders still output (xb, ab, yb) triples\n","    ang_tr = np.zeros(len(ytr), dtype=np.float32)\n","    ang_va = np.zeros(len(yva), dtype=np.float32)\n","\n","# ---------------------------\n","# PyTorch Tensor Conversion\n","# ---------------------------\n","Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n","Xva_t = torch.tensor(Xva, dtype=torch.float32)\n","ytr_t = torch.tensor(ytr, dtype=torch.long)\n","yva_t = torch.tensor(yva, dtype=torch.long)\n","ang_tr_t = torch.tensor(ang_tr, dtype=torch.float32).view(-1, 1)\n","ang_va_t = torch.tensor(ang_va, dtype=torch.float32).view(-1, 1)\n","\n","# ---------------------------\n","# Lightweight Augmentations\n","# ---------------------------\n","# We implement custom tensor-based flips/rotations (works for 2-channel SAR)\n","import random\n","\n","def aug_batch(xb):\n","    \"\"\"Apply in-place random flips/rot90 to a batch of images [B,2,H,W].\"\"\"\n","    B = xb.size(0)\n","    for i in range(B):\n","        if random.random() < cfg.hflip_p:\n","            xb[i] = torch.flip(xb[i], dims=[2])  # horizontal flip (W axis)\n","        if random.random() < cfg.vflip_p:\n","            xb[i] = torch.flip(xb[i], dims=[1])  # vertical flip (H axis)\n","        if random.random() < cfg.rot90_p:\n","            k = random.randint(1,3)\n","            xb[i] = torch.rot90(xb[i], k, dims=[1,2])  # rotate k*90°\n","    return xb"],"metadata":{"id":"t69VltVVOdBV","executionInfo":{"status":"ok","timestamp":1760980381158,"user_tz":300,"elapsed":640,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ---------------------------------------\n","# DataLoaders (always yield (xb, ab, yb))\n","# ---------------------------------------\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","train_ds = TensorDataset(Xtr_t, ang_tr_t, ytr_t)\n","val_ds   = TensorDataset(Xva_t, ang_va_t, yva_t)\n","\n","train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=2)\n","val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size*2, shuffle=False, num_workers=2)\n"],"metadata":{"id":"v4JtI2hil1ws","executionInfo":{"status":"ok","timestamp":1760980778340,"user_tz":300,"elapsed":6,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# CNN with Angle Fusion\n","# ===========================\n","class IcebergVesselCNN(nn.Module):\n","    \"\"\"\n","    A compact CNN trunk for 2-channel SAR (band_1, band_2) + optional angle fusion.\n","    Trunk: Conv -> ReLU -> Pool x3 + GlobalAvgPool -> feature vector (64 dims)\n","    Head:  If use_angle, concat [feat, angle] -> Linear -> logits(2)\n","    \"\"\"\n","    def __init__(self, in_ch=2, use_angle=True):\n","        super().__init__()\n","        self.use_angle = use_angle\n","        self.trunk = nn.Sequential(\n","            nn.Conv2d(in_ch, 32, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # 75->37\n","            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # 37->18\n","            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # 18->9\n","            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d(1),          # -> [B,128,1,1]\n","        )\n","        feat_dim = 128\n","        head_in = feat_dim + (1 if use_angle else 0)\n","        self.head = nn.Sequential(\n","            nn.Linear(head_in, 64), nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(64, 2)\n","        )\n","\n","    def forward(self, x, angle=None):\n","        f = self.trunk(x).view(x.size(0), -1)  # [B,128]\n","        if self.use_angle:\n","            assert angle is not None, \"Angle tensor is required when use_angle=True\"\n","            f = torch.cat([f, angle], dim=1)   # [B,129]\n","        return self.head(f)\n","\n","model = IcebergVesselCNN(in_ch=2, use_angle=cfg.use_angle).to(device)\n"],"metadata":{"id":"PAlFFgkKOhaL","executionInfo":{"status":"ok","timestamp":1760980781578,"user_tz":300,"elapsed":8,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# =============================\n","# Training Setup (model/opt/lr)\n","# =============================\n","from pathlib import Path\n","import numpy as np\n","import torch, torch.nn as nn, torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Model (2 SAR bands; angle fusion controlled by cfg.use_angle)\n","model = IcebergVesselCNN(in_ch=2, use_angle=cfg.use_angle).to(device)\n","\n","# Loss / Optimizer / Scheduler\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n","scheduler = ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=cfg.lr_patience)\n","\n","# ===========================\n","# AMP (Mixed Precision)\n","# ===========================\n","try:\n","    from torch.amp import autocast, GradScaler   # modern API\n","    use_amp = torch.cuda.is_available()\n","    scaler  = GradScaler('cuda' if use_amp else 'cpu')\n","except Exception:\n","    # Fallback for older torch\n","    from torch.cuda.amp import autocast, GradScaler\n","    use_amp = torch.cuda.is_available()\n","    scaler  = GradScaler() if use_amp else None\n","\n","# ===========================\n","# One Epoch (train/eval)\n","# ===========================\n","def run_epoch(loader, train=True):\n","    \"\"\"One epoch. Returns (avg_loss, accuracy, auc).\"\"\"\n","    model.train() if train else model.eval()\n","    losses, probs_all, ys_all = [], [], []\n","\n","    for xb, ab, yb in loader:\n","        xb, ab, yb = xb.to(device), ab.to(device), yb.to(device)\n","\n","        # Apply augmentation only during training\n","        if train:\n","            xb = aug_batch(xb)\n","\n","        if train:\n","            optimizer.zero_grad(set_to_none=True)\n","            with autocast(device_type='cuda', enabled=use_amp):\n","                logits = model(xb, ab) if cfg.use_angle else model(xb)\n","                loss   = criterion(logits, yb)\n","\n","            if use_amp:\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","        else:\n","            with torch.no_grad():\n","                with autocast(device_type='cuda', enabled=use_amp):\n","                    logits = model(xb, ab) if cfg.use_angle else model(xb)\n","                    loss   = criterion(logits, yb)\n","\n","        # ---- bookkeeping\n","        losses.append(loss.item())\n","        prob = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n","        y_np = yb.detach().cpu().numpy()\n","        probs_all.append(prob); ys_all.append(y_np)\n","\n","    from sklearn.metrics import accuracy_score, roc_auc_score\n","    y_true = np.concatenate(ys_all)\n","    y_prob = np.concatenate(probs_all)\n","    y_pred = (y_prob >= 0.5).astype(np.int64)\n","\n","    acc = accuracy_score(y_true, y_pred)\n","    try:\n","        auc = roc_auc_score(y_true, y_prob)\n","    except Exception:\n","        auc = float(\"nan\")\n","\n","    return float(np.mean(losses)), acc, auc\n","\n","# ===========================\n","# Training Loop\n","# ===========================\n","best_auc = 0.0\n","epochs_no_improve = 0\n","best_path = str(Path(cfg.save_dir) / \"best_iceberg_vessel_cnn.pt\")\n","\n","for ep in range(1, cfg.epochs + 1):\n","    tr_loss, tr_acc, tr_auc = run_epoch(train_loader, train=True)\n","    va_loss, va_acc, va_auc = run_epoch(val_loader,   train=False)\n","\n","    if np.isfinite(va_auc):\n","        scheduler.step(va_auc)\n","\n","    print(f\"Epoch {ep:02d} | \"\n","          f\"tr loss {tr_loss:.4f} acc {tr_acc:.3f} auc {tr_auc:.3f} || \"\n","          f\"va loss {va_loss:.4f} acc {va_acc:.3f} auc {va_auc:.3f}\")\n","\n","    if va_auc > best_auc + 1e-4:\n","        best_auc = va_auc\n","        epochs_no_improve = 0\n","        torch.save({\"model_state\": model.state_dict(), \"cfg\": dict(cfg.__dict__)}, best_path)\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= cfg.patience:\n","            print(f\"Early stopping at epoch {ep} (best val AUC={best_auc:.3f}).\")\n","            break\n","\n","print(\"Best model saved to:\", best_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QYHIXCOOlxq","executionInfo":{"status":"ok","timestamp":1760981195912,"user_tz":300,"elapsed":409124,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"60033358-b492-4955-ddea-a881f097c0db"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 01 | tr loss 0.6542 acc 0.576 auc 0.616 || va loss 0.6571 acc 0.620 auc 0.663\n","Epoch 02 | tr loss 0.6752 acc 0.595 auc 0.622 || va loss 0.6384 acc 0.614 auc 0.711\n","Epoch 03 | tr loss 0.6542 acc 0.588 auc 0.638 || va loss 0.6406 acc 0.688 auc 0.712\n","Epoch 04 | tr loss 0.6354 acc 0.631 auc 0.676 || va loss 0.6138 acc 0.676 auc 0.726\n","Epoch 05 | tr loss 0.6245 acc 0.652 auc 0.697 || va loss 0.5982 acc 0.692 auc 0.722\n","Epoch 06 | tr loss 0.6054 acc 0.680 auc 0.723 || va loss 0.5856 acc 0.664 auc 0.771\n","Epoch 07 | tr loss 0.5917 acc 0.669 auc 0.735 || va loss 0.5618 acc 0.710 auc 0.798\n","Epoch 08 | tr loss 0.5437 acc 0.701 auc 0.771 || va loss 0.4941 acc 0.741 auc 0.818\n","Epoch 09 | tr loss 0.4847 acc 0.759 auc 0.819 || va loss 0.4656 acc 0.788 auc 0.865\n","Epoch 10 | tr loss 0.4471 acc 0.791 auc 0.857 || va loss 0.4245 acc 0.785 auc 0.878\n","Epoch 11 | tr loss 0.4373 acc 0.804 auc 0.871 || va loss 0.4778 acc 0.769 auc 0.872\n","Epoch 12 | tr loss 0.4207 acc 0.805 auc 0.877 || va loss 0.4117 acc 0.819 auc 0.892\n","Epoch 13 | tr loss 0.4186 acc 0.821 auc 0.888 || va loss 0.5162 acc 0.769 auc 0.881\n","Epoch 14 | tr loss 0.5210 acc 0.774 auc 0.844 || va loss 0.4140 acc 0.816 auc 0.890\n","Epoch 15 | tr loss 0.4051 acc 0.811 auc 0.884 || va loss 0.4039 acc 0.822 auc 0.888\n","Epoch 16 | tr loss 0.3943 acc 0.829 auc 0.896 || va loss 0.4056 acc 0.813 auc 0.898\n","Epoch 17 | tr loss 0.3677 acc 0.846 auc 0.907 || va loss 0.3660 acc 0.835 auc 0.909\n","Epoch 18 | tr loss 0.3622 acc 0.845 auc 0.911 || va loss 0.4619 acc 0.798 auc 0.896\n","Epoch 19 | tr loss 0.3792 acc 0.829 auc 0.898 || va loss 0.3664 acc 0.829 auc 0.919\n","Epoch 20 | tr loss 0.3551 acc 0.849 auc 0.913 || va loss 0.3641 acc 0.832 auc 0.918\n","Best model saved to: /content/artifacts/best_iceberg_vessel_cnn.pt\n"]}]},{"cell_type":"code","source":["# ===========================\n","# Load Best & Full Evaluation\n","# ===========================\n","ckpt = torch.load(best_path, map_location=device)\n","model.load_state_dict(ckpt[\"model_state\"])\n","model.eval()\n","\n","# Collect validation predictions for metrics & confusion matrix\n","all_probs, all_preds, all_true = [], [], []\n","with torch.no_grad():\n","    for xb, ab, yb in val_loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        ab = ab.to(device) if cfg.use_angle else None\n","        logits = model(xb, ab) if cfg.use_angle else model(xb)\n","        prob = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n","        pred = (prob >= 0.5).astype(int)\n","        all_probs.append(prob)\n","        all_preds.append(pred)\n","        all_true.append(yb.cpu().numpy())\n","\n","y_true = np.concatenate(all_true)\n","y_prob = np.concatenate(all_probs)\n","y_pred = np.concatenate(all_preds)\n","\n","acc = accuracy_score(y_true, y_pred)\n","auc = roc_auc_score(y_true, y_prob)\n","cm  = confusion_matrix(y_true, y_pred)\n","\n","print(f\"Validation ACC: {acc:.3f} | AUC: {auc:.3f}\")\n","print(\"Confusion matrix [[TN, FP],[FN, TP]]:\\n\", cm)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixlOs5CpQ02q","executionInfo":{"status":"ok","timestamp":1760981392213,"user_tz":300,"elapsed":3505,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"aabfd0dd-db8b-4d74-a361-bab56d85051d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation ACC: 0.829 | AUC: 0.919\n","Confusion matrix [[TN, FP],[FN, TP]]:\n"," [[131  39]\n"," [ 16 135]]\n"]}]}]}