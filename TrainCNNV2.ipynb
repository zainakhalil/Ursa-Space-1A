{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNI3+MZ+H3OtGDHvP4SQRzU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["The notebook mounts Google Drive, reads train.json from “Ursa Space 1A,” and converts each sample’s band_1 and band_2 lists into 75×75 float images, stacked as a two-channel tensor [N, 2, 75, 75]. Labels (is_iceberg) and incidence angles are loaded; angles are imputed with the train-set median for missing values and optionally z-normalized. A stratified 80/20 split creates train/validation indices. Per-channel statistics (mean, std) are computed on the train tensor and used to z-normalize both splits; all arrays plus angles are saved to a compact artifact sar_fullres_splits.npz for reproducibility. The model IcebergVesselCNN is a compact convolutional trunk (Conv-ReLU-MaxPool blocks → AdaptiveAvgPool to a 128-d feature) with an angle-fusion head that concatenates the (optionally normalized) angle to the pooled feature before a small MLP classifier. Training uses AdamW with cross-entropy loss, optional mixed precision (AMP), light SAR-appropriate augmentations (random horizontal/vertical flips and 90° rotations), and a ReduceLROnPlateau scheduler keyed to validation AUC, with early stopping based on AUC to prevent overfitting. After each epoch, accuracy and AUC are computed on the validation set and the best checkpoint is stored as best_iceberg_vessel_cnn.pt. Final reporting includes accuracy, ROC-AUC, and a confusion matrix (e.g., TN/FP/FN/TP), confirming strong performance on the iceberg-vs-vessel classification task."],"metadata":{"id":"vQhyZrb2TP_-"}},{"cell_type":"code","source":["# ==== GitHub setup (Colab) — safe, idempotent, and data-friendly =====================\n","# - Uses Colab secret: GITHUB_TOKEN (Settings → Secrets → Add secret)\n","# - Clones your repo without exposing the token\n","# - Sets a push-URL that authenticates via the secret\n","# - Writes a .gitignore to keep large data/artifacts out of Git\n","# - Leaves optional \"copy & push\" lines commented — un-comment when ready\n","\n","from google.colab import userdata\n","import os, pathlib, textwrap, subprocess, sys\n","\n","# ---- EDIT THESE THREE VALUES --------------------------------------------------------\n","GITHUB_USER  = \"zainakhalil\"\n","GITHUB_REPO  = \"Ursa-Space-1A\"\n","GITHUB_EMAIL = \"zkhal4@uic.edu\"     # <-- set to your GitHub email\n","# ------------------------------------------------------------------------------------\n","\n","# 0) Load token from Colab secrets (won't print)\n","token = userdata.get(\"GITHUB_TOKEN\")\n","if not token:\n","    raise RuntimeError(\n","        \"No GITHUB_TOKEN found in Colab secrets. \"\n","        \"Add one in Colab: Settings → Secrets → New secret → key: GITHUB_TOKEN\"\n","    )\n","os.environ[\"GITHUB_TOKEN\"] = token  # used only for push URL; won't be printed\n","\n","repo_https = f\"https://github.com/{GITHUB_USER}/{GITHUB_REPO}.git\"\n","push_https = f\"https://{GITHUB_USER}:{token}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\"\n","repo_dir   = pathlib.Path(GITHUB_REPO)\n","\n","# 1) Clone if needed (fetch URL stays clean; push URL uses token)\n","if not repo_dir.exists():\n","    print(f\"Cloning {repo_https} …\")\n","    subprocess.run([\"git\", \"clone\", repo_https, GITHUB_REPO], check=True)\n","else:\n","    print(f\"Repo already exists at: {repo_dir.resolve()}\")\n","\n","# 2) Configure git identity and push URL\n","os.chdir(repo_dir)\n","subprocess.run([\"git\", \"config\", \"user.name\", GITHUB_USER], check=True)\n","subprocess.run([\"git\", \"config\", \"user.email\", GITHUB_EMAIL], check=True)\n","subprocess.run([\"git\", \"remote\", \"set-url\", \"--push\", \"origin\", push_https], check=True)\n","\n","# 3) Write a .gitignore (keeps data/artifacts/checkpoints out of Git)\n","gitignore = textwrap.dedent(\"\"\"\n","# ---- data & artifacts (keep out of Git) ----\n","data/\n","artifacts/\n","*.npz\n","*.pt\n","*.7z\n","*.json\n","\n","# ---- notebooks noise ----\n","*.ipynb_checkpoints/\n","\n","# ---- OS/editor ----\n",".DS_Store\n","*.swp\n","\"\"\").lstrip()\n","\n","gi_path = pathlib.Path(\".gitignore\")\n","if gi_path.exists():\n","    print(\".gitignore already exists; leaving as-is.\")\n","else:\n","    gi_path.write_text(gitignore)\n","    subprocess.run([\"git\", \"add\", \".gitignore\"], check=True)\n","    subprocess.run([\"git\", \"commit\", \"-m\", \"Add .gitignore for data/artifacts\"], check=False)\n","\n","print(\"\\nGit setup complete. Current remotes:\")\n","subprocess.run([\"git\", \"remote\", \"-v\"], check=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uNYA5sGazC2","executionInfo":{"status":"ok","timestamp":1760977186048,"user_tz":300,"elapsed":5079,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"777b446e-4238-4527-8112-ec3a9d7b9609"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning https://github.com/zainakhalil/Ursa-Space-1A.git …\n",".gitignore already exists; leaving as-is.\n","\n","Git setup complete. Current remotes:\n"]},{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['git', 'remote', '-v'], returncode=0)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","# Get the GitHub token from Colab Secrets\n","github_token = userdata.get('GITHUB_TOKEN')\n","\n","# Replace with your GitHub username and repository name\n","github_username = 'zainakhalil'\n","repository_name = 'Ursa-Space-1A'\n","\n","# Construct the clone URL with the token\n","clone_url = f'https://{github_token}@github.com/{github_username}/{repository_name}.git'\n","\n","# Clone the repository\n","!git clone {clone_url}"],"metadata":{"id":"vh85WN-sgPxE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add All Changes\n","!git add \"TrainCNNV2.ipynb\""],"metadata":{"id":"AE_-LW6RcJ63","executionInfo":{"status":"ok","timestamp":1760978547614,"user_tz":300,"elapsed":127,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Replace 'Your_Notebook_Name.ipynb' with the actual name of your notebook file\n","!cp \"/content/drive/MyDrive/Colab Notebooks/TrainCNNV2.ipynb\" \"/content/Ursa-Space-1A/\""],"metadata":{"id":"pBXbpc0wel-h","executionInfo":{"status":"ok","timestamp":1760978549492,"user_tz":300,"elapsed":817,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Commit Changes\n","!git commit -m \"Train CNN Attempt 2\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G9x6OkCQerOp","executionInfo":{"status":"ok","timestamp":1760978549790,"user_tz":300,"elapsed":124,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"c7b372fa-e390-448f-e5c5-63006fbda20f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[main c4d20a7] Train CNN Attempt 2\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite TrainCNNV2.ipynb (82%)\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"zkhal4@uic.edu\"\n","!git config --global user.name \"zainakhalil\""],"metadata":{"id":"YSC8xOsFe44z","executionInfo":{"status":"ok","timestamp":1760978552690,"user_tz":300,"elapsed":216,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Push to the remote repository\n","# 'origin' is the default remote name, and 'main' or 'master' is typically the branch name\n","!git push origin main\n","# If your branch is named 'master', use:\n","# !git push origin master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUaCtIKCe764","executionInfo":{"status":"ok","timestamp":1760978555690,"user_tz":300,"elapsed":799,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"dd3dab50-f10f-4f6e-9298-565e54c244cc"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Invalid username or token. Password authentication is not supported for Git operations.\n","fatal: Authentication failed for 'https://github.com/zainakhalil/Ursa-Space-1A.git/'\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qk-BQ8P7KT2s","executionInfo":{"status":"ok","timestamp":1760973143943,"user_tz":300,"elapsed":546,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"c758ba33-0d0b-4c88-82d4-adb51b4569f9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","!ls \"/content/drive/MyDrive/Ursa Space 1A/train.json\"\n","df = pd.read_json('/content/drive/MyDrive/Ursa Space 1A/train.json')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2tnLJgYKaVn","executionInfo":{"status":"ok","timestamp":1760973251578,"user_tz":300,"elapsed":4224,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"4d1da594-24a6-41ec-bd4a-467259a0923e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["'/content/drive/MyDrive/Ursa Space 1A/train.json'\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZim7rvJJdUr","executionInfo":{"status":"ok","timestamp":1760973373360,"user_tz":300,"elapsed":11660,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"9392560a-0b17-416f-f3ff-2fe64e3425bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1283, 2, 75, 75), (321, 2, 75, 75))"]},"metadata":{},"execution_count":4}],"source":["import math, numpy as np, pandas as pd\n","from sklearn.model_selection import train_test_split\n","from pathlib import Path\n","\n","DATA_PATH = Path(\"/content/drive/MyDrive/Ursa Space 1A/train.json\")\n","ART_DIR   = Path(\"/content/artifacts\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n","\n","df = pd.read_json(DATA_PATH)\n","\n","def to_img(seq):\n","    a = np.array(seq, dtype=np.float32)\n","    side = int(round(math.sqrt(a.size)))\n","    assert side*side == a.size, f\"Non-square band length {a.size}\"\n","    return a.reshape(side, side)\n","\n","# [N, 2, H, W]  (two channels: band_1, band_2)\n","X = np.stack([np.stack([to_img(b1), to_img(b2)], axis=0)\n","              for b1, b2 in zip(df[\"band_1\"], df[\"band_2\"])], axis=0)\n","y = df[\"is_iceberg\"].astype(np.int64).to_numpy()\n","inc_angle = pd.to_numeric(df[\"inc_angle\"], errors=\"coerce\").to_numpy()\n","\n","# stratified 80/20 split\n","idx = np.arange(len(y))\n","idx_tr, idx_va = train_test_split(idx, test_size=0.2, random_state=1234, stratify=y)\n","X_tr, X_va = X[idx_tr], X[idx_va]\n","y_tr, y_va = y[idx_tr], y[idx_va]\n","ang_tr, ang_va = inc_angle[idx_tr], inc_angle[idx_va]\n","\n","# per-channel z-normalization using train stats\n","m = X_tr.mean(axis=(0,2,3), keepdims=True)\n","s = X_tr.std(axis=(0,2,3), keepdims=True) + 1e-6\n","X_tr = (X_tr - m)/s\n","X_va = (X_va - m)/s\n","\n","np.savez_compressed(ART_DIR / \"sar_fullres_splits.npz\",\n","    X_train=X_tr, y_train=y_tr, X_val=X_va, y_val=y_va,\n","    means=m.squeeze(), stds=s.squeeze(), side=int(X.shape[-1]),\n","    inc_angle_train=ang_tr, inc_angle_val=ang_va\n",")\n","X_tr.shape, X_va.shape\n"]},{"cell_type":"code","source":["import numpy as np, torch, torch.nn as nn, torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from pathlib import Path\n","\n","data = np.load(\"/content/artifacts/sar_fullres_splits.npz\")\n","Xtr, Xva = data[\"X_train\"], data[\"X_val\"]\n","ytr, yva = data[\"y_train\"], data[\"y_val\"]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n","Xva_t = torch.tensor(Xva, dtype=torch.float32)\n","ytr_t = torch.tensor(ytr, dtype=torch.long)\n","yva_t = torch.tensor(yva, dtype=torch.long)\n","\n","train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=64, shuffle=True)\n","val_loader   = DataLoader(TensorDataset(Xva_t, yva_t), batch_size=128, shuffle=False)\n","\n","class SmallCNN(nn.Module):\n","    def __init__(self, in_ch=2, num_classes=2):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(in_ch, 16, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d(1),\n","        )\n","        self.fc = nn.Linear(64, num_classes)\n","    def forward(self, x):\n","        x = self.net(x).view(x.size(0), -1)\n","        return self.fc(x)\n","\n","model = SmallCNN().to(device)\n","opt = optim.Adam(model.parameters(), lr=1e-3)\n","crit = nn.CrossEntropyLoss()\n","\n","def run_epoch(dl, train=True):\n","    model.train() if train else model.eval()\n","    tot=correct=loss_sum=0\n","    for xb,yb in dl:\n","        xb,yb = xb.to(device), yb.to(device)\n","        if train: opt.zero_grad()\n","        with torch.set_grad_enabled(train):\n","            logits = model(xb); loss = crit(logits, yb)\n","            if train: loss.backward(); opt.step()\n","        pred = logits.argmax(1)\n","        correct += (pred==yb).sum().item()\n","        tot += yb.numel(); loss_sum += loss.item()*yb.size(0)\n","    return loss_sum/tot, correct/tot\n","\n","for ep in range(8):\n","    tr_loss,tr_acc = run_epoch(train_loader, True)\n","    va_loss,va_acc = run_epoch(val_loader, False)\n","    print(f\"Epoch {ep+1}: tr {tr_loss:.4f}/{tr_acc:.3f} | va {va_loss:.4f}/{va_acc:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNhSmZRTMYwK","executionInfo":{"status":"ok","timestamp":1760973461784,"user_tz":300,"elapsed":64055,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"807c61d2-c461-403f-d4d6-ff3e37cf3ccc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: tr 0.6810/0.535 | va 0.6508/0.639\n","Epoch 2: tr 0.6617/0.599 | va 0.6393/0.664\n","Epoch 3: tr 0.6517/0.627 | va 0.6335/0.651\n","Epoch 4: tr 0.6467/0.627 | va 0.6277/0.642\n","Epoch 5: tr 0.6396/0.649 | va 0.6323/0.626\n","Epoch 6: tr 0.6363/0.646 | va 0.7098/0.576\n","Epoch 7: tr 0.6781/0.612 | va 0.6370/0.651\n","Epoch 8: tr 0.6403/0.641 | va 0.6154/0.664\n"]}]},{"cell_type":"code","source":["# ==================================\n","# Config Small CNN + Small Utilities\n","# ==================================\n","from dataclasses import dataclass\n","from pathlib import Path\n","import numpy as np\n","import torch, torch.nn as nn, torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n","\n","@dataclass\n","class Config:\n","    # Paths\n","    npz_path: str = \"/content/artifacts/sar_fullres_splits.npz\"\n","    save_dir: str = \"/content/artifacts\"\n","    # Training\n","    batch_size: int = 64\n","    epochs: int = 20\n","    lr: float = 1e-3\n","    weight_decay: float = 1e-4\n","    # Early stopping / LR schedule\n","    patience: int = 5          # epochs with no AUC improvement to stop\n","    lr_patience: int = 2       # plateaus before LR reduce\n","    lr_gamma: float = 0.5\n","    # Augmentations\n","    hflip_p: float = 0.5\n","    vflip_p: float = 0.5\n","    rot90_p: float = 0.5       # random k*90° rotation\n","    # Incidence angle fusion\n","    use_angle: bool = True     # set False to ignore angle\n","    norm_angle: bool = True    # z-norm angle using train stats\n","\n","cfg = Config()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","Path(cfg.save_dir).mkdir(parents=True, exist_ok=True)\n","print(\"Using device:\", device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNQMvn5KOT98","executionInfo":{"status":"ok","timestamp":1760973915175,"user_tz":300,"elapsed":48,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"c5f68a49-c125-4c2d-cc35-e6362f28902d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# ===========================\n","# Load NPZ + Prepare Tensors\n","# ===========================\n","D = np.load(cfg.npz_path)\n","\n","Xtr = D[\"X_train\"]          # [N, 2, 75, 75]\n","Xva = D[\"X_val\"]\n","ytr = D[\"y_train\"].astype(np.int64)\n","yva = D[\"y_val\"].astype(np.int64)\n","\n","# Angle arrays can have NaN; we'll impute with train median, then optionally z-norm\n","if cfg.use_angle:\n","    ang_tr = D[\"inc_angle_train\"].astype(np.float32)\n","    ang_va = D[\"inc_angle_val\"].astype(np.float32)\n","\n","    # Impute missing angles with train median (robust/simple)\n","    train_med = np.nanmedian(ang_tr)\n","    ang_tr = np.where(np.isfinite(ang_tr), ang_tr, train_med)\n","    ang_va = np.where(np.isfinite(ang_va), ang_va, train_med)\n","\n","    if cfg.norm_angle:\n","        mu, sd = ang_tr.mean(), ang_tr.std() + 1e-6\n","        ang_tr = (ang_tr - mu) / sd\n","        ang_va = (ang_va - mu) / sd\n","else:\n","    ang_tr = np.zeros(len(ytr), dtype=np.float32)\n","    ang_va = np.zeros(len(yva), dtype=np.float32)\n","\n","# ---------------------------\n","# PyTorch Tensor Conversion\n","# ---------------------------\n","Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n","Xva_t = torch.tensor(Xva, dtype=torch.float32)\n","ytr_t = torch.tensor(ytr, dtype=torch.long)\n","yva_t = torch.tensor(yva, dtype=torch.long)\n","ang_tr_t = torch.tensor(ang_tr, dtype=torch.float32).view(-1, 1)\n","ang_va_t = torch.tensor(ang_va, dtype=torch.float32).view(-1, 1)\n","\n","# ---------------------------\n","# Lightweight Augmentations\n","# ---------------------------\n","# We implement custom tensor-based flips/rotations (works for 2-channel SAR)\n","import random\n","\n","def aug_batch(xb):\n","    \"\"\"Apply in-place random flips/rot90 to a batch of images [B,2,H,W].\"\"\"\n","    B = xb.size(0)\n","    for i in range(B):\n","        if random.random() < cfg.hflip_p:\n","            xb[i] = torch.flip(xb[i], dims=[2])  # horizontal flip (W axis)\n","        if random.random() < cfg.vflip_p:\n","            xb[i] = torch.flip(xb[i], dims=[1])  # vertical flip (H axis)\n","        if random.random() < cfg.rot90_p:\n","            k = random.randint(1,3)\n","            xb[i] = torch.rot90(xb[i], k, dims=[1,2])  # rotate k*90°\n","    return xb\n"],"metadata":{"id":"t69VltVVOdBV","executionInfo":{"status":"ok","timestamp":1760973950331,"user_tz":300,"elapsed":708,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","# CNN with Angle Fusion\n","# ===========================\n","class IcebergVesselCNN(nn.Module):\n","    \"\"\"\n","    A compact CNN trunk for 2-channel SAR (band_1, band_2) + optional angle fusion.\n","    Trunk: Conv -> ReLU -> Pool x3 + GlobalAvgPool -> feature vector (64 dims)\n","    Head:  If use_angle, concat [feat, angle] -> Linear -> logits(2)\n","    \"\"\"\n","    def __init__(self, in_ch=2, use_angle=True):\n","        super().__init__()\n","        self.use_angle = use_angle\n","        self.trunk = nn.Sequential(\n","            nn.Conv2d(in_ch, 32, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # 75->37\n","            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # 37->18\n","            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n","            nn.MaxPool2d(2),  # 18->9\n","            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d(1),          # -> [B,128,1,1]\n","        )\n","        feat_dim = 128\n","        head_in = feat_dim + (1 if use_angle else 0)\n","        self.head = nn.Sequential(\n","            nn.Linear(head_in, 64), nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(64, 2)\n","        )\n","\n","    def forward(self, x, angle=None):\n","        f = self.trunk(x).view(x.size(0), -1)  # [B,128]\n","        if self.use_angle:\n","            assert angle is not None, \"Angle tensor is required when use_angle=True\"\n","            f = torch.cat([f, angle], dim=1)   # [B,129]\n","        return self.head(f)\n","\n","model = IcebergVesselCNN(in_ch=2, use_angle=cfg.use_angle).to(device)\n"],"metadata":{"id":"PAlFFgkKOhaL","executionInfo":{"status":"ok","timestamp":1760973967784,"user_tz":300,"elapsed":3,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import numpy as np, torch\n","from torch.amp import autocast, GradScaler  # new API (works on Colab)\n","# If your torch is older, fallback:\n","# from torch.cuda.amp import autocast, GradScaler\n","\n","use_amp = torch.cuda.is_available()\n","scaler  = GradScaler('cuda' if use_amp else 'cpu')\n","\n","def run_epoch(loader, train=True):\n","    \"\"\"One epoch. Returns (avg_loss, accuracy, auc).\"\"\"\n","    model.train() if train else model.eval()\n","    losses, preds_all, probs_all, ys_all = [], [], [], []\n","\n","    for xb, ab, yb in loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        ab = ab.to(device) if cfg.use_angle else None\n","\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        if train:\n","            # ---- forward with (optional) autocast\n","            with autocast(device_type='cuda', enabled=use_amp):\n","                logits = model(xb, ab) if cfg.use_angle else model(xb)\n","                loss   = criterion(logits, yb)\n","\n","            # ---- backward + step (AMP or not)\n","            if use_amp:\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","            else:\n","                loss.backward()\n","                optimizer.step()\n","        else:\n","            with torch.no_grad():\n","                logits = model(xb, ab) if cfg.use_angle else model(xb)\n","                loss   = criterion(logits, yb)\n","\n","        # ---- bookkeeping\n","        losses.append(loss.item())\n","        prob = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n","        pred = (prob >= 0.5).astype(np.int64)\n","        y_np = yb.detach().cpu().numpy()\n","        probs_all.append(prob); preds_all.append(pred); ys_all.append(y_np)\n","\n","    from sklearn.metrics import accuracy_score, roc_auc_score\n","    y_true = np.concatenate(ys_all)\n","    y_prob = np.concatenate(probs_all)\n","    y_pred = np.concatenate(preds_all)\n","    acc = accuracy_score(y_true, y_pred)\n","    try:\n","        auc = roc_auc_score(y_true, y_prob)\n","    except Exception:\n","        auc = float(\"nan\")\n","    return float(np.mean(losses)), acc, auc\n","\n","# -------- training loop (unchanged except for using the fixed run_epoch) --------\n","best_auc = 0.0\n","epochs_no_improve = 0\n","best_path = str(Path(cfg.save_dir) / \"best_iceberg_vessel_cnn.pt\")\n","\n","for ep in range(1, cfg.epochs+1):\n","    tr_loss, tr_acc, tr_auc = run_epoch(train_loader, train=True)\n","    va_loss, va_acc, va_auc = run_epoch(val_loader,   train=False)\n","\n","    if np.isfinite(va_auc):\n","        scheduler.step(va_auc)\n","\n","    print(f\"Epoch {ep:02d} | \"\n","          f\"tr loss {tr_loss:.4f} acc {tr_acc:.3f} auc {tr_auc:.3f} || \"\n","          f\"va loss {va_loss:.4f} acc {va_acc:.3f} auc {va_auc:.3f}\")\n","\n","    if va_auc > best_auc + 1e-4:\n","        best_auc = va_auc\n","        epochs_no_improve = 0\n","        torch.save({\"model_state\": model.state_dict(), \"cfg\": cfg.__dict__}, best_path)\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= cfg.patience:\n","            print(f\"Early stopping at epoch {ep} (best val AUC={best_auc:.3f}).\")\n","            break\n","\n","print(\"Best model saved to:\", best_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QYHIXCOOlxq","executionInfo":{"status":"ok","timestamp":1760974783722,"user_tz":300,"elapsed":422616,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"25f75908-26bd-4bf6-e5d5-ad69c2f903ef"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 01 | tr loss 0.6630 acc 0.562 auc 0.614 || va loss 0.6458 acc 0.636 auc 0.681\n","Epoch 02 | tr loss 0.6473 acc 0.622 auc 0.645 || va loss 0.6273 acc 0.660 auc 0.688\n","Epoch 03 | tr loss 0.6220 acc 0.629 auc 0.671 || va loss 0.6159 acc 0.623 auc 0.727\n","Epoch 04 | tr loss 0.6261 acc 0.640 auc 0.688 || va loss 0.5832 acc 0.701 auc 0.749\n","Epoch 05 | tr loss 0.5788 acc 0.675 auc 0.747 || va loss 0.5449 acc 0.701 auc 0.757\n","Epoch 06 | tr loss 0.5368 acc 0.738 auc 0.777 || va loss 0.4808 acc 0.763 auc 0.819\n","Epoch 07 | tr loss 0.4963 acc 0.761 auc 0.822 || va loss 0.4691 acc 0.757 auc 0.818\n","Epoch 08 | tr loss 0.4333 acc 0.792 auc 0.847 || va loss 0.4113 acc 0.791 auc 0.877\n","Epoch 09 | tr loss 0.5107 acc 0.794 auc 0.877 || va loss 0.4542 acc 0.779 auc 0.883\n","Epoch 10 | tr loss 0.5172 acc 0.735 auc 0.792 || va loss 0.4569 acc 0.785 auc 0.847\n","Epoch 11 | tr loss 0.4566 acc 0.800 auc 0.864 || va loss 0.4034 acc 0.807 auc 0.885\n","Epoch 12 | tr loss 0.4465 acc 0.794 auc 0.844 || va loss 0.4257 acc 0.810 auc 0.877\n","Epoch 13 | tr loss 0.3896 acc 0.815 auc 0.884 || va loss 0.4159 acc 0.798 auc 0.913\n","Epoch 14 | tr loss 0.3774 acc 0.823 auc 0.891 || va loss 0.3702 acc 0.826 auc 0.904\n","Epoch 15 | tr loss 0.3725 acc 0.840 auc 0.917 || va loss 0.3561 acc 0.838 auc 0.920\n","Epoch 16 | tr loss 0.3204 acc 0.845 auc 0.936 || va loss 0.3146 acc 0.860 auc 0.942\n","Epoch 17 | tr loss 0.3057 acc 0.865 auc 0.940 || va loss 0.3278 acc 0.854 auc 0.938\n","Epoch 18 | tr loss 0.3094 acc 0.865 auc 0.944 || va loss 0.3212 acc 0.847 auc 0.940\n","Epoch 19 | tr loss 0.2661 acc 0.879 auc 0.951 || va loss 0.3636 acc 0.819 auc 0.955\n","Epoch 20 | tr loss 0.2775 acc 0.872 auc 0.954 || va loss 0.2498 acc 0.875 auc 0.962\n","Best model saved to: /content/artifacts/best_iceberg_vessel_cnn.pt\n"]}]},{"cell_type":"code","source":["# ===========================\n","# Load Best & Full Evaluation\n","# ===========================\n","ckpt = torch.load(best_path, map_location=device)\n","model.load_state_dict(ckpt[\"model_state\"])\n","model.eval()\n","\n","# Collect validation predictions for metrics & confusion matrix\n","all_probs, all_preds, all_true = [], [], []\n","with torch.no_grad():\n","    for xb, ab, yb in val_loader:\n","        xb, yb = xb.to(device), yb.to(device)\n","        ab = ab.to(device) if cfg.use_angle else None\n","        logits = model(xb, ab) if cfg.use_angle else model(xb)\n","        prob = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n","        pred = (prob >= 0.5).astype(int)\n","        all_probs.append(prob)\n","        all_preds.append(pred)\n","        all_true.append(yb.cpu().numpy())\n","\n","y_true = np.concatenate(all_true)\n","y_prob = np.concatenate(all_probs)\n","y_pred = np.concatenate(all_preds)\n","\n","acc = accuracy_score(y_true, y_pred)\n","auc = roc_auc_score(y_true, y_prob)\n","cm  = confusion_matrix(y_true, y_pred)\n","\n","print(f\"Validation ACC: {acc:.3f} | AUC: {auc:.3f}\")\n","print(\"Confusion matrix [[TN, FP],[FN, TP]]:\\n\", cm)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixlOs5CpQ02q","executionInfo":{"status":"ok","timestamp":1760974858272,"user_tz":300,"elapsed":1896,"user":{"displayName":"Zaina Khalil","userId":"08515068669285487272"}},"outputId":"0b1a9dd4-6903-4282-c7de-975adb73f17c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation ACC: 0.875 | AUC: 0.962\n","Confusion matrix [[TN, FP],[FN, TP]]:\n"," [[142  28]\n"," [ 12 139]]\n"]}]}]}